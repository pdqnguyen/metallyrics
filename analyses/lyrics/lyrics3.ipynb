{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song lyrics - Part 4: Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\philippe\\Anaconda3\\envs\\metallyrics\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "from nlp import get_stopwords, tokenize, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('songs-ml-1pct.csv')\n",
    "# genres = df.columns[1:]\n",
    "# X = df.lyrics.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\philippe\\Anaconda3\\envs\\metallyrics\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['insermi'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57878, 6370)\n"
     ]
    }
   ],
   "source": [
    "# vectorizer = TfidfVectorizer(stop_words=get_stopwords(), min_df=50)\n",
    "# X_v = vectorizer.fit_transform(X)\n",
    "# print(X_v.shape)\n",
    "# vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['death', 'life', 'flesh', 'blood', 'dead', 'human', 'pain', 'mind', 'kill']\n",
      "1\n",
      "['night', 'light', 'eyes', 'sun', 'black', 'dark', 'cold', 'time', 'sky']\n",
      "2\n",
      "['gonna', 'yeah', 'time', 'wanna', 'fuck', 'rock', 'metal', 'fucking', 'kill']\n",
      "3\n",
      "['blood', 'death', 'god', 'evil', 'black', 'rise', 'gods', 'fight', 'lord']\n",
      "4\n",
      "['time', 'life', 'feel', 'love', 'day', 'eyes', 'mind', 'pain', 'heart']\n"
     ]
    }
   ],
   "source": [
    "# n_components = 5\n",
    "\n",
    "# lda = LDA(n_components=5)\n",
    "# X_lda = lda.fit_transform(X_v)\n",
    "# for i in range(n_components):\n",
    "#     topic_words = lda.components_[i].argsort()[:-10:-1]\n",
    "#     print(i)\n",
    "#     print([vocabulary[word] for word in topic_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('songs-1pct.csv')\n",
    "\n",
    "tokenize_kwargs = dict(english_only=True, stopwords=get_stopwords())\n",
    "df.loc[:, 'song_words'] = (df.song_darklyrics\n",
    "                           .apply(tokenize, **tokenize_kwargs)\n",
    "                           .apply(lemmatize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [embrace, midwinter, frost, light, darkness, s...\n",
       "1        [watch, sun, feel, coldness, crawling, sitting...\n",
       "2        [spellbind, night, anger, rage, force, hate, b...\n",
       "3        [lost, hall, dream, bound, scream, stand, allo...\n",
       "4        [sane, wander, hope, master, understand, sickn...\n",
       "                               ...                        \n",
       "58169    [withdraw, twitch, hey, personal, faith, reque...\n",
       "58170    [recite, unspoken, manifest, solid, faith, acc...\n",
       "58171    [telling, fable, demise, lie, underdog, narrat...\n",
       "58172    [yard, ruin, shimmer, paint, pale, stale, floc...\n",
       "58173    [vast, deceiver, underdog, god, crushing, bone...\n",
       "Name: song_words, Length: 58174, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['song_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_words = df.groupby('band_name').song_words.sum()\n",
    "texts = list(band_words.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(texts)\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "num_topics = 5\n",
    "\n",
    "lda_model = LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"life\" + 0.010*\"time\" + 0.008*\"death\" + 0.007*\"night\" + 0.007*\"blood\" + 0.006*\"soul\" + 0.006*\"feel\" + 0.006*\"god\" + 0.006*\"light\" + 0.006*\"pain\"'),\n",
       " (1,\n",
       "  '0.013*\"life\" + 0.010*\"time\" + 0.007*\"light\" + 0.007*\"soul\" + 0.006*\"day\" + 0.006*\"love\" + 0.006*\"fall\" + 0.005*\"dream\" + 0.005*\"mind\" + 0.005*\"death\"'),\n",
       " (2,\n",
       "  '0.012*\"time\" + 0.010*\"life\" + 0.007*\"death\" + 0.007*\"night\" + 0.007*\"blood\" + 0.006*\"mind\" + 0.005*\"feel\" + 0.005*\"god\" + 0.005*\"soul\" + 0.005*\"dream\"'),\n",
       " (3,\n",
       "  '0.011*\"life\" + 0.007*\"mind\" + 0.006*\"soul\" + 0.006*\"time\" + 0.006*\"night\" + 0.005*\"death\" + 0.005*\"light\" + 0.005*\"fear\" + 0.005*\"blood\" + 0.005*\"lie\"'),\n",
       " (4,\n",
       "  '0.012*\"life\" + 0.010*\"time\" + 0.008*\"death\" + 0.007*\"blood\" + 0.006*\"night\" + 0.006*\"soul\" + 0.006*\"god\" + 0.006*\"feel\" + 0.005*\"dead\" + 0.005*\"lie\"')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LdaMulticore in module gensim.models.ldamulticore object:\n",
      "\n",
      "class LdaMulticore(gensim.models.ldamodel.LdaModel)\n",
      " |  LdaMulticore(corpus=None, num_topics=100, id2word=None, workers=None, chunksize=2000, passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, random_state=None, minimum_probability=0.01, minimum_phi_value=0.01, per_word_topics=False, dtype=<class 'numpy.float32'>)\n",
      " |  \n",
      " |  The constructor estimates Latent Dirichlet Allocation model parameters based\n",
      " |  on a training corpus:\n",
      " |  \n",
      " |  >>> lda = LdaMulticore(corpus, num_topics=10)\n",
      " |  \n",
      " |  You can then infer topic distributions on new, unseen documents, with\n",
      " |  \n",
      " |  >>> doc_lda = lda[doc_bow]\n",
      " |  \n",
      " |  The model can be updated (trained) with new documents via\n",
      " |  \n",
      " |  >>> lda.update(other_corpus)\n",
      " |  \n",
      " |  Model persistency is achieved through its `load`/`save` methods.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaMulticore\n",
      " |      gensim.models.ldamodel.LdaModel\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      gensim.models.basemodel.BaseTopicModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, corpus=None, num_topics=100, id2word=None, workers=None, chunksize=2000, passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, random_state=None, minimum_probability=0.01, minimum_phi_value=0.01, per_word_topics=False, dtype=<class 'numpy.float32'>)\n",
      " |      If given, start training from the iterable `corpus` straight away. If not given,\n",
      " |      the model is left untrained (presumably because you want to call `update()` manually).\n",
      " |      \n",
      " |      `num_topics` is the number of requested latent topics to be extracted from\n",
      " |      the training corpus.\n",
      " |      \n",
      " |      `id2word` is a mapping from word ids (integers) to words (strings). It is\n",
      " |      used to determine the vocabulary size, as well as for debugging and topic\n",
      " |      printing.\n",
      " |      \n",
      " |      `workers` is the number of extra processes to use for parallelization. Uses\n",
      " |      all available cores by default: `workers=cpu_count()-1`. **Note**: for\n",
      " |      hyper-threaded CPUs, `cpu_count()` returns a useless number -- set `workers`\n",
      " |      directly to the number of your **real** cores (not hyperthreads) minus one,\n",
      " |      for optimal performance.\n",
      " |      \n",
      " |      If `batch` is not set, perform online training by updating the model once\n",
      " |      every `workers * chunksize` documents (online training). Otherwise,\n",
      " |      run batch LDA, updating model only once at the end of each full corpus pass.\n",
      " |      \n",
      " |      `alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\n",
      " |      (theta) and topic-word (lambda) distributions. Both default to a symmetric\n",
      " |      1.0/num_topics prior.\n",
      " |      \n",
      " |      `alpha` can be set to an explicit array = prior of your choice. It also\n",
      " |      support special values of 'asymmetric' and 'auto': the former uses a fixed\n",
      " |      normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\n",
      " |      prior directly from your data.\n",
      " |      \n",
      " |      `eta` can be a scalar for a symmetric prior over topic/word\n",
      " |      distributions, or a matrix of shape num_topics x num_words,\n",
      " |      which can be used to impose asymmetric priors over the word\n",
      " |      distribution on a per-topic basis. This may be useful if you\n",
      " |      want to seed certain topics with particular words by boosting\n",
      " |      the priors for those words.\n",
      " |      \n",
      " |      Calculate and log perplexity estimate from the latest mini-batch once every\n",
      " |      `eval_every` documents. Set to `None` to disable perplexity estimation (faster),\n",
      " |      or to `0` to only evaluate perplexity once, at the end of each corpus pass.\n",
      " |      \n",
      " |      `decay` and `offset` parameters are the same as Kappa and Tau_0 in\n",
      " |      Hoffman et al, respectively.\n",
      " |      \n",
      " |      `random_state` can be a numpy.random.RandomState object or the seed for one\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> lda = LdaMulticore(corpus, id2word=id2word, num_topics=100)  # train model\n",
      " |      >>> print(lda[doc_bow]) # get topic probability distribution for a document\n",
      " |      >>> lda.update(corpus2) # update the LDA model with additional documents\n",
      " |      >>> print(lda[doc_bow])\n",
      " |  \n",
      " |  update(self, corpus, chunks_as_numpy=False)\n",
      " |      Train the model with new documents, by EM-iterating over `corpus` until\n",
      " |      the topics converge (or until the maximum number of allowed iterations\n",
      " |      is reached). `corpus` must be an iterable (repeatable stream of documents),\n",
      " |      \n",
      " |      The E-step is distributed into the several processes.\n",
      " |      \n",
      " |      This update also supports updating an already trained model (`self`)\n",
      " |      with new documents from `corpus`; the two models are then merged in\n",
      " |      proportion to the number of old vs. new documents. This feature is still\n",
      " |      experimental for non-stationary input streams.\n",
      " |      \n",
      " |      For stationary input (no topic drift in new documents), on the other hand,\n",
      " |      this equals the online update of Hoffman et al. and is guaranteed to\n",
      " |      converge for any `decay` in (0.5, 1.0>.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __slotnames__ = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.ldamodel.LdaModel:\n",
      " |  \n",
      " |  __getitem__(self, bow, eps=None)\n",
      " |      Args:\n",
      " |          bow (list): Bag-of-words representation of a document.\n",
      " |          eps (float): Ignore topics with probability below `eps`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topic distribution for the given document `bow`, as a list of\n",
      " |          `(topic_id, topic_probability)` 2-tuples.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      " |      Estimate the variational bound of documents from `corpus`:\n",
      " |      E_q[log p(corpus)] - E_q[log q(corpus)]\n",
      " |      \n",
      " |      Args:\n",
      " |          corpus: documents to infer variational bounds from.\n",
      " |          gamma: the variational parameters on topic weights for each `corpus`\n",
      " |              document (=2d matrix=what comes out of `inference()`).\n",
      " |              If not supplied, will be inferred from the model.\n",
      " |          subsample_ratio (float): If `corpus` is a sample of the whole corpus,\n",
      " |              pass this to inform on what proportion of the corpus it represents.\n",
      " |              This is used as a multiplicative factor to scale the likelihood\n",
      " |              appropriately.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The variational bound score calculated.\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear model state (free up some memory). Used in the distributed algo.\n",
      " |  \n",
      " |  diff(self, other, distance='kullback_leibler', num_words=100, n_ann_terms=10, diagonal=False, annotation=True, normed=True)\n",
      " |      Calculate difference topic2topic between two Lda models\n",
      " |      `other` instances of `LdaMulticore` or `LdaModel`\n",
      " |      `distance` is function that will be applied to calculate difference between any topic pair.\n",
      " |      Available values: `kullback_leibler`, `hellinger`, `jaccard` and `jensen_shannon`\n",
      " |      `num_words` is quantity of most relevant words that used if distance == `jaccard` (also used for annotation)\n",
      " |      `n_ann_terms` is max quantity of words in intersection/symmetric difference between topics (used for annotation)\n",
      " |      `diagonal` set to True if the difference is required only between the identical topic no.s\n",
      " |      (returns diagonal of diff matrix)\n",
      " |      `annotation` whether the intersection or difference of words between two topics should be returned\n",
      " |      Returns a matrix Z with shape (m1.num_topics, m2.num_topics),\n",
      " |      where Z[i][j] - difference between topic_i and topic_j\n",
      " |      and matrix annotation (if True) with shape (m1.num_topics, m2.num_topics, 2, None),\n",
      " |      where::\n",
      " |      \n",
      " |          annotation[i][j] = [[`int_1`, `int_2`, ...], [`diff_1`, `diff_2`, ...]] and\n",
      " |          `int_k` is word from intersection of `topic_i` and `topic_j` and\n",
      " |          `diff_l` is word from symmetric difference of `topic_i` and `topic_j`\n",
      " |          `normed` is a flag. If `true`, matrix Z will be normalized\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> m1, m2 = LdaMulticore.load(path_1), LdaMulticore.load(path_2)\n",
      " |      >>> mdiff, annotation = m1.diff(m2)\n",
      " |      >>> print(mdiff) # get matrix with difference for each topic pair from `m1` and `m2`\n",
      " |      >>> print(annotation) # get array with positive/negative words for each topic pair from `m1` and `m2`\n",
      " |      \n",
      " |      Note: this ignores difference in model dtypes\n",
      " |  \n",
      " |  do_estep(self, chunk, state=None)\n",
      " |      Perform inference on a chunk of documents, and accumulate the collected\n",
      " |      sufficient statistics in `state` (or `self.state` if None).\n",
      " |  \n",
      " |  do_mstep(self, rho, other, extra_pass=False)\n",
      " |      M step: use linear interpolation between the existing topics and\n",
      " |      collected sufficient statistics in `other` to update the topics.\n",
      " |  \n",
      " |  get_document_topics(self, bow, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
      " |      Args:\n",
      " |          bow (list): Bag-of-words representation of the document to get topics for.\n",
      " |          minimum_probability (float): Ignore topics with probability below this value\n",
      " |              (None by default). If set to None, a value of 1e-8 is used to prevent 0s.\n",
      " |          per_word_topics (bool): If True, also returns a list of topics, sorted in\n",
      " |              descending order of most likely topics for that word. It also returns a list\n",
      " |              of word_ids and each words corresponding topics' phi_values, multiplied by\n",
      " |              feature length (i.e, word count).\n",
      " |          minimum_phi_value (float): if `per_word_topics` is True, this represents a lower\n",
      " |              bound on the term probabilities that are included (None by default). If set\n",
      " |              to None, a value of 1e-8 is used to prevent 0s.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topic distribution for the given document `bow`, as a list of\n",
      " |          `(topic_id, topic_probability)` 2-tuples.\n",
      " |  \n",
      " |  get_term_topics(self, word_id, minimum_probability=None)\n",
      " |      Args:\n",
      " |          word_id (int): ID of the word to get topic probabilities for.\n",
      " |          minimum_probability (float): Only include topic probabilities above this\n",
      " |              value (None by default). If set to None, use 1e-8 to prevent including 0s.\n",
      " |      Returns:\n",
      " |          list: The most likely topics for the given word. Each topic is represented\n",
      " |          as a tuple of `(topic_id, term_probability)`.\n",
      " |  \n",
      " |  get_topic_terms(self, topicid, topn=10)\n",
      " |      Args:\n",
      " |          topn (int): Only return 2-tuples for the topn most probable words\n",
      " |              (ignore the rest).\n",
      " |      \n",
      " |      Returns:\n",
      " |          list: `(word_id, probability)` 2-tuples for the most probable words\n",
      " |          in topic with id `topicid`.\n",
      " |  \n",
      " |  get_topics(self)\n",
      " |      Returns:\n",
      " |          np.ndarray: `num_topics` x `vocabulary_size` array of floats (self.dtype) which represents\n",
      " |          the term topic matrix learned during inference.\n",
      " |  \n",
      " |  inference(self, chunk, collect_sstats=False)\n",
      " |      Given a chunk of sparse document vectors, estimate gamma (parameters\n",
      " |      controlling the topic weights) for each document in the chunk.\n",
      " |      \n",
      " |      This function does not modify the model (=is read-only aka const). The\n",
      " |      whole input chunk of document is assumed to fit in RAM; chunking of a\n",
      " |      large corpus must be done earlier in the pipeline.\n",
      " |      \n",
      " |      If `collect_sstats` is True, also collect sufficient statistics needed\n",
      " |      to update the model's topic-word distributions, and return a 2-tuple\n",
      " |      `(gamma, sstats)`. Otherwise, return `(gamma, None)`. `gamma` is of shape\n",
      " |      `len(chunk) x self.num_topics`.\n",
      " |      \n",
      " |      Avoids computing the `phi` variational parameter directly using the\n",
      " |      optimization presented in **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\n",
      " |  \n",
      " |  init_dir_prior(self, prior, name)\n",
      " |  \n",
      " |  log_perplexity(self, chunk, total_docs=None)\n",
      " |      Calculate and return per-word likelihood bound, using the `chunk` of\n",
      " |      documents as evaluation corpus. Also output the calculated statistics. incl.\n",
      " |      perplexity=2^(-bound), to log at INFO level.\n",
      " |  \n",
      " |  save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs)\n",
      " |      Save the model to file.\n",
      " |      \n",
      " |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      " |      \n",
      " |      `separately` can be used to define which arrays should be stored in separate files.\n",
      " |      \n",
      " |      `ignore` parameter can be used to define which variables should be ignored, i.e. left\n",
      " |      out from the pickled lda model. By default the internal `state` is ignored as it uses\n",
      " |      its own serialisation not the one provided by `LdaModel`. The `state` and `dispatcher`\n",
      " |      will be added to any ignore parameter defined.\n",
      " |      \n",
      " |      \n",
      " |      Note: do not save as a compressed file if you intend to load the file back with `mmap`.\n",
      " |      \n",
      " |      Note: If you intend to use models across Python 2/3 versions there are a few things to\n",
      " |      keep in mind:\n",
      " |      \n",
      " |        1. The pickled Python dictionaries will not work across Python versions\n",
      " |        2. The `save` method does not automatically save all np arrays using np, only\n",
      " |           those ones that exceed `sep_limit` set in `gensim.utils.SaveLoad.save`. The main\n",
      " |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      " |      \n",
      " |      Please refer to the wiki recipes section (goo.gl/qoje24)\n",
      " |      for an example on how to work around these issues.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10)\n",
      " |      Args:\n",
      " |          topn (int): Only return 2-tuples for the topn most probable words\n",
      " |              (ignore the rest).\n",
      " |      \n",
      " |      Returns:\n",
      " |          list: of `(word, probability)` 2-tuples for the most probable\n",
      " |          words in topic `topicid`.\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      Args:\n",
      " |          num_topics (int): show results for first `num_topics` topics.\n",
      " |              Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      " |              The returned `num_topics <= self.num_topics` subset of all topics is\n",
      " |              therefore arbitrary and may change between two LDA training runs.\n",
      " |          num_words (int): include top `num_words` with highest probabilities in topic.\n",
      " |          log (bool): If True, log output in addition to returning it.\n",
      " |          formatted (bool): If True, format topics as strings, otherwise return them as\n",
      " |              `(word, probability)` 2-tuples.\n",
      " |      Returns:\n",
      " |          list: `num_words` most significant words for `num_topics` number of topics\n",
      " |          (10 words for top 10 topics, by default).\n",
      " |  \n",
      " |  sync_state(self)\n",
      " |  \n",
      " |  top_topics(self, corpus=None, texts=None, dictionary=None, window_size=None, coherence='u_mass', topn=20, processes=-1)\n",
      " |      Calculate the coherence for each topic; default is Umass coherence.\n",
      " |      \n",
      " |      See the :class:`gensim.models.CoherenceModel` constructor for more info on the\n",
      " |      parameters and the different coherence metrics.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list: tuples with `(topic_repr, coherence_score)`, where `topic_repr` is a list\n",
      " |          of representations of the `topn` terms for the topic. The terms are represented\n",
      " |          as tuples of `(membership_in_topic, token)`. The `coherence_score` is a float.\n",
      " |  \n",
      " |  update_alpha(self, gammat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-document\n",
      " |      topic weights `alpha` given the last `gammat`.\n",
      " |  \n",
      " |  update_eta(self, lambdat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-topic\n",
      " |      word weights `eta` given the last `lambdat`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.models.ldamodel.LdaModel:\n",
      " |  \n",
      " |  load(fname, *args, **kwargs) from builtins.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      " |      \n",
      " |          >>> LdaModel.load(fname, mmap='r')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.basemodel.BaseTopicModel:\n",
      " |  \n",
      " |  print_topic(self, topicno, topn=10)\n",
      " |      Get a single topic as a formatted string.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topicno : int\n",
      " |          Topic id.\n",
      " |      topn : int\n",
      " |          Number of words from topic that will be used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          String representation of topic, like '-0.340 * \"category\" + 0.298 * \"$M$\" + 0.183 * \"algebra\" + ... '.\n",
      " |  \n",
      " |  print_topics(self, num_topics=20, num_words=10)\n",
      " |      Get the most significant topics (alias for `show_topics()` method).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_topics : int, optional\n",
      " |          The number of topics to be selected, if -1 - all topics will be in result (ordered by significance).\n",
      " |      num_words : int, optional\n",
      " |          The number of words to be included per topics (ordered by significance).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (int, list of (str, float))\n",
      " |          Sequence with (topic_id, [(word, value), ... ]).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1883620119029663449523297467\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1883620119029663449523297467_data = {\"mdsDat\": {\"x\": [0.005386717190288707, 0.0014823088796701845, -0.0017814287304279094, -0.0018546732988325963, -0.0032329240406983903], \"y\": [-0.0021284742935986996, 0.002939514123819991, 0.0008849820878009639, 0.0023229761946105807, -0.004018998112632832], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [25.96828710101352, 23.984368884078002, 17.216672791807706, 16.454074649859873, 16.376596573240892]}, \"tinfo\": {\"Term\": [\"time\", \"mind\", \"death\", \"blood\", \"black\", \"night\", \"dead\", \"cold\", \"kill\", \"darkness\", \"lie\", \"fear\", \"head\", \"god\", \"deep\", \"human\", \"hope\", \"hand\", \"love\", \"hate\", \"left\", \"life\", \"king\", \"beast\", \"feel\", \"soul\", \"body\", \"sun\", \"dark\", \"living\", \"aam\", \"totum\", \"yom\", \"alimony\", \"meddlesome\", \"kore\", \"wa\", \"dragonhead\", \"desireless\", \"triplicity\", \"nauseousness\", \"unwire\", \"myst\", \"pollen\", \"hedgehog\", \"bruin\", \"unrighteousness\", \"hoot\", \"mou\", \"wayless\", \"undreaming\", \"fife\", \"howdy\", \"pimply\", \"rufus\", \"unfaith\", \"thankfully\", \"greyness\", \"aeonian\", \"epigenetic\", \"metal\", \"district\", \"fairy\", \"believing\", \"skylark\", \"warrior\", \"love\", \"rock\", \"comfort\", \"colors\", \"lullaby\", \"news\", \"day\", \"ready\", \"light\", \"moonlight\", \"chance\", \"beware\", \"matter\", \"land\", \"hope\", \"wrong\", \"eternal\", \"unite\", \"change\", \"hero\", \"pray\", \"answer\", \"heart\", \"voice\", \"life\", \"shine\", \"fall\", \"dream\", \"power\", \"star\", \"start\", \"rise\", \"steel\", \"break\", \"wind\", \"time\", \"live\", \"soul\", \"lost\", \"fear\", \"deep\", \"burn\", \"fight\", \"pain\", \"mind\", \"sky\", \"feel\", \"god\", \"dark\", \"blood\", \"night\", \"death\", \"hear\", \"lie\", \"dead\", \"volar\", \"rumbo\", \"alto\", \"fuerte\", \"thereabove\", \"hoy\", \"calor\", \"th\", \"izar\", \"ahunt\", \"inthronization\", \"guardo\", \"agua\", \"dolor\", \"automatically\", \"gladius\", \"dado\", \"caza\", \"precedence\", \"senicide\", \"centripetal\", \"ripper\", \"whack\", \"ectoplasmic\", \"unbenign\", \"signum\", \"freeman\", \"condivision\", \"hieron\", \"misgovernment\", \"murder\", \"aggressor\", \"domine\", \"living\", \"holy\", \"rite\", \"blast\", \"evil\", \"bring\", \"field\", \"killing\", \"gate\", \"school\", \"chaos\", \"illusion\", \"legion\", \"dark\", \"rule\", \"death\", \"pain\", \"waiting\", \"kingdom\", \"dawn\", \"born\", \"flame\", \"hear\", \"iron\", \"coming\", \"god\", \"feel\", \"night\", \"final\", \"life\", \"father\", \"time\", \"dream\", \"blood\", \"light\", \"lie\", \"soul\", \"live\", \"control\", \"fall\", \"fear\", \"mind\", \"heart\", \"black\", \"day\", \"dead\", \"sky\", \"left\", \"break\", \"burn\", \"love\", \"lost\", \"puna\", \"chasse\", \"handbreadth\", \"upholster\", \"puist\", \"ree\", \"windup\", \"resuscitation\", \"dissimulate\", \"articulation\", \"ann\", \"hunchback\", \"undigestable\", \"impassiveness\", \"turma\", \"ventrotomy\", \"pinta\", \"sant\", \"perorate\", \"linge\", \"unembalmed\", \"colation\", \"colporrhagia\", \"druidess\", \"skrike\", \"quatre\", \"orlo\", \"reinsertion\", \"washtub\", \"cleavage\", \"rais\", \"betray\", \"coroner\", \"tartar\", \"wheel\", \"rotting\", \"trap\", \"snake\", \"desire\", \"victim\", \"true\", \"pure\", \"peace\", \"ship\", \"heaven\", \"wisdom\", \"follow\", \"twist\", \"body\", \"choose\", \"laugh\", \"hand\", \"king\", \"cold\", \"city\", \"head\", \"breath\", \"blood\", \"hate\", \"dead\", \"cover\", \"cut\", \"death\", \"god\", \"life\", \"time\", \"soul\", \"night\", \"love\", \"feel\", \"lie\", \"sky\", \"hear\", \"black\", \"kill\", \"burning\", \"day\", \"flesh\", \"fear\", \"heart\", \"dark\", \"stand\", \"live\", \"fall\", \"lost\", \"pain\", \"mind\", \"light\", \"dream\", \"sarcous\", \"bramble\", \"uta\", \"methodology\", \"manticore\", \"thujone\", \"comedic\", \"griddle\", \"untested\", \"streamlet\", \"vitiation\", \"welsh\", \"hemic\", \"bor\", \"mischance\", \"tideless\", \"rollick\", \"electrolytic\", \"irritant\", \"soldering\", \"fulguration\", \"antre\", \"teratoma\", \"harem\", \"refrigerate\", \"amorousness\", \"blench\", \"parcel\", \"anthology\", \"mirthful\", \"coaxial\", \"atavistic\", \"irreligious\", \"bedizen\", \"hypocrite\", \"sigil\", \"stork\", \"taunt\", \"doom\", \"corpse\", \"time\", \"pee\", \"hew\", \"voluntary\", \"flowing\", \"dry\", \"church\", \"wraith\", \"coffin\", \"mountainside\", \"tower\", \"ghost\", \"shore\", \"surface\", \"reality\", \"build\", \"slave\", \"reject\", \"remain\", \"return\", \"break\", \"stone\", \"sleep\", \"word\", \"living\", \"beg\", \"blacken\", \"grave\", \"sacrifice\", \"night\", \"blood\", \"cold\", \"red\", \"nightmare\", \"mind\", \"death\", \"beast\", \"darkness\", \"spirit\", \"black\", \"dark\", \"dream\", \"dead\", \"land\", \"sun\", \"sky\", \"truth\", \"change\", \"feel\", \"life\", \"head\", \"pain\", \"love\", \"god\", \"hope\", \"fall\", \"rise\", \"hear\", \"soul\", \"light\", \"lie\", \"heart\", \"lost\", \"fight\", \"live\", \"kill\", \"day\", \"culpable\", \"golpe\", \"incalculably\", \"stapler\", \"kat\", \"vesperal\", \"sailorman\", \"sah\", \"ultimo\", \"dinero\", \"subdivide\", \"subjectively\", \"fiesta\", \"lehr\", \"obstructive\", \"essentially\", \"hexagon\", \"solipsism\", \"unchangeability\", \"unbeheld\", \"placer\", \"unburnt\", \"wort\", \"shapelessness\", \"fand\", \"unearned\", \"contortedly\", \"quirk\", \"boden\", \"sayid\", \"adad\", \"overrule\", \"buggy\", \"jazz\", \"nihilist\", \"gimmick\", \"faith\", \"glimpse\", \"pull\", \"kill\", \"mind\", \"deep\", \"disease\", \"fading\", \"erase\", \"grab\", \"days\", \"left\", \"hard\", \"crush\", \"fear\", \"darkness\", \"black\", \"human\", \"feast\", \"lie\", \"money\", \"friend\", \"beast\", \"air\", \"sun\", \"fight\", \"lost\", \"mirror\", \"dead\", \"soul\", \"play\", \"hand\", \"life\", \"waiting\", \"night\", \"shadow\", \"light\", \"hold\", \"pain\", \"fall\", \"feel\", \"hate\", \"hope\", \"blood\", \"stand\", \"cold\", \"god\", \"death\", \"time\", \"day\", \"burn\", \"sky\", \"dream\", \"break\", \"live\"], \"Freq\": [33239.0, 19186.0, 23120.0, 20753.0, 12982.0, 21656.0, 15353.0, 9333.0, 10461.0, 9873.0, 15464.0, 14957.0, 8766.0, 18208.0, 9440.0, 5678.0, 7774.0, 7261.0, 15091.0, 9078.0, 9889.0, 41501.0, 5509.0, 3754.0, 18720.0, 21151.0, 7507.0, 9126.0, 13684.0, 6287.0, 2.0463266040885846, 5.841249053258687, 1.6026210544903765, 2.0591627546075193, 6.6644046371539485, 2.544041945217947, 26.568439975832543, 6.9600604279640725, 2.945232974560104, 2.947406750766424, 0.9762498047572229, 0.9700711722198685, 4.308761075167202, 3.3819023431714665, 1.9030151844005392, 2.3752875641457014, 0.939200034222303, 18.231379813737338, 4.166674753558625, 2.3319914705148554, 1.852095517383538, 16.597076748976114, 3.6536017380881796, 1.3746727954072084, 9.536588587700976, 0.9043158564082642, 3.1775584890771364, 4.060571924641432, 3.1527451509183404, 2.6997162876990295, 1485.871240254423, 8.510798993014776, 107.30133733700815, 194.50320032576954, 7.110282548528894, 981.4093658362834, 5444.24096420616, 1207.3012770817736, 247.86786001733617, 174.092868301687, 76.04814655265328, 189.56827329521997, 5500.209519249263, 1081.9077204040154, 6653.688630904562, 300.2515812721423, 1138.4268651969478, 365.29736666940886, 977.3264976519553, 2999.2657606750354, 2670.7668664737994, 1361.322293632584, 1605.7627272844647, 549.7591579824344, 2006.1069626914277, 615.0405631007148, 1498.5938775457983, 1091.723984557108, 4297.762473969767, 1822.7469198828746, 11985.769021315453, 1075.168117011945, 5430.436286667987, 4870.715147572175, 2776.112210964348, 2172.6658451519534, 1673.152140058234, 2915.1649868585473, 1109.3894114910183, 3450.87970936496, 2305.500536430649, 8835.629254536816, 3659.5066696106974, 5843.085199466042, 3514.906200228181, 4183.22283931635, 2789.16559523622, 2919.3625871046115, 3114.153833185917, 4228.151045666718, 4753.592024148986, 3456.418919508766, 4441.931741158129, 4266.681154969283, 3418.842669186551, 4519.138452639199, 4628.805750791689, 4732.346832742401, 3230.979418818706, 3505.207047611505, 3331.3555925344, 4.929324335380714, 7.613214487028582, 5.285499498592052, 4.2451643618209065, 1.3180224483391954, 10.359602143728479, 1.7125100952975651, 1.6262859183164675, 1.623871372621949, 1.199812819254938, 0.7942462184277252, 0.797947538937803, 5.977107649510666, 9.020701657366622, 1.9964377515041452, 6.125800218146949, 0.7643358890030147, 0.7633144939971328, 3.787897407059023, 1.4584516645050898, 2.9738127414875457, 27.077346208595458, 34.472668568683176, 2.1775488340216165, 1.0973324493120553, 4.831754518350725, 3.9814380437847428, 1.0899713322072677, 0.7244460563450178, 1.4436986372796239, 733.1743956725645, 28.630407081620888, 31.743318967612183, 2157.845463334747, 1408.3662277072688, 348.72176037104765, 232.41367429453592, 2824.547052493005, 2813.125762402165, 222.30856056407637, 1051.3149901301326, 431.04782968834525, 112.85005950536839, 924.0655938460638, 605.8554855548048, 469.28774792049876, 4181.829546517727, 1354.0439615638434, 6843.158499710893, 4925.064767725678, 1505.323931836163, 859.7341000644249, 1222.3530374014802, 1838.0602336806296, 2119.84303875708, 3717.3812035937212, 509.0064714407882, 1798.3449500201518, 5108.888200606454, 5240.722356709302, 5952.404126072883, 1507.6989639326903, 10430.61352277722, 955.3814087084552, 8359.940222372563, 4231.133057196154, 5384.867771077557, 5091.227195920851, 4117.176234740389, 5322.485148982464, 3280.6879428688826, 1581.8438044466095, 4383.0953618827525, 3785.4854591752996, 4580.58994427442, 3370.6769687779574, 3271.8543828729153, 3753.9828408148105, 3643.0160209485425, 3173.105945057144, 2541.135664899126, 2799.9801470291427, 2509.426081639978, 3081.7468994264727, 2550.3424496846424, 1.1511241016981466, 1.0868107017556259, 1.056870475299424, 1.0445201687264345, 1.041627945583638, 2.4035303359344997, 2.3868750852593417, 3.357186310057107, 1.6688054523370137, 1.5842494376104221, 8.907395677258103, 4.0110906406160325, 0.6289972113810964, 0.623918958119763, 0.6161294675404977, 0.6219747336369631, 0.9667652556702612, 0.9200781469295694, 0.918183355288646, 0.6159769491381227, 1.5077453496073667, 0.6112322001040289, 0.6055981287913527, 2.0977213607223453, 1.4956986062313746, 0.5984511311921403, 0.8840783114310962, 0.5994446777281208, 0.5961737373929537, 2.3613355874092155, 8.3904578287125, 393.5011026665704, 13.935158314277603, 3.159605518470679, 378.7417217773531, 322.5733414507337, 548.0641095409796, 248.42616633632954, 874.1329214570432, 665.3529467295018, 1073.0053136268848, 537.9849154471501, 784.5982034427254, 266.60159076887874, 1085.4101115352719, 392.22718537618596, 978.1478258130893, 533.6495978778462, 1687.705215531757, 494.1804878371649, 523.0409252655692, 1605.6010986985357, 1221.9126846806978, 1991.7136114013877, 502.5824912486532, 1875.8400206679962, 998.9840178376935, 4128.66600288079, 1913.2380291342522, 3081.488294031207, 524.8182912670622, 722.3659754420814, 4448.376785716569, 3549.8445722833317, 7332.8926234495575, 5667.058481979626, 3726.3023852705264, 3768.448038544074, 2754.3341523221966, 3292.785032186093, 2786.6382749602244, 2398.4236238243943, 2319.828283866578, 2336.086796466197, 1948.380395323462, 1462.9674942261518, 2667.78706631857, 1593.1737347648143, 2472.461312562081, 2228.3768940834293, 2264.131492315603, 1769.1401708356084, 2076.1318089723995, 2637.7689026238436, 1919.9664085300176, 2225.2280761774923, 2374.668526711242, 2360.671006388597, 2020.231443065216, 1.2015057077727693, 1.076597613143002, 1.0689004833464288, 1.7502951416754442, 1.399341128915502, 0.6898604192474994, 0.6814833834672654, 0.6766618187574117, 0.6729594967006348, 2.3231252471288126, 1.6486503173850364, 1.3391356813164712, 0.6616832742059653, 5.761148304200585, 0.6449460386565794, 2.5383301824576527, 0.6472356081332403, 0.6388501863550908, 0.6380225331715157, 0.6346342059221821, 0.6293265093942397, 1.540481698838449, 1.8595375520467115, 3.1185941229791507, 0.6174475192936809, 0.6270978824367809, 0.6170523531476364, 2.1737236370609194, 1.5251045953326414, 0.6201377602279949, 1.221886755439278, 7.107255374410064, 4.1394292030217565, 1.820089561791614, 58.297437698728054, 11.415980079104115, 2.6788275271359123, 25.537357343411216, 499.32466283959195, 539.9087834831712, 6850.243121161981, 17.020038029438147, 16.67613683297082, 5.989372917392234, 172.32189488493958, 354.5492291074044, 259.9289929444142, 30.979346089696495, 156.2303985871886, 20.97426419690984, 245.94689852743838, 437.4299736205334, 349.7046040555579, 180.0992992944741, 729.35875877802, 547.0751812679179, 579.3473568387469, 126.23850976946534, 474.67738584066524, 1036.2151221045078, 2246.2014198006855, 835.0282936123937, 823.5839645072984, 499.12227222119986, 1276.3780900680042, 233.42426170577787, 298.5505801057632, 774.515629140723, 641.6416487168235, 3930.843110769927, 3750.219824622621, 1810.20699382306, 879.8357738549252, 559.4948528867154, 3456.1906673196418, 4055.2907585489, 780.8968168777258, 1840.7865610837669, 686.8137270547385, 2293.166531526416, 2399.4603144122702, 2649.824490610518, 2573.7659978913475, 1587.8011068132075, 1634.043446406665, 2211.743556654457, 1371.0852312876207, 1114.9610344778387, 2909.846255481306, 5497.445480066757, 1538.0461182554882, 2550.385047035287, 2351.682100425638, 2707.2112606599553, 1376.3225433028606, 2571.284213660635, 1555.5288656907612, 1913.126179450672, 2680.903298988528, 2467.7262930741203, 2108.624176387632, 1854.677473201088, 1737.028576961274, 1639.7828838103694, 1682.4820273859068, 1595.174746036026, 1676.4474766011494, 1.0524156341884614, 1.8877015795889707, 0.933863584853246, 0.9283261530751518, 21.83259743682692, 0.8645462622115309, 2.48667774650841, 3.517999073006709, 0.7805123215110634, 1.9359151011813986, 4.483002723491599, 1.4288386008513576, 1.4411909264911809, 0.7048984169303308, 0.6867485797006726, 1.6495070826093468, 1.341667519099817, 0.6646736526125443, 0.6575670768845369, 0.6738939343068653, 1.6245317750239003, 0.9788208205518878, 1.615062627150094, 0.645136956670942, 1.9226321714920205, 0.9803091753785068, 0.6469758012193602, 2.1781769609787327, 0.6411227311788057, 0.9594906848135345, 1.5500507021562364, 16.085551779536203, 2.8215287838512064, 6.212548508027528, 11.45145300706214, 4.538347095777314, 953.7013673306151, 88.41057651520089, 456.4988626785372, 2275.549918898856, 4021.3409047161613, 2059.306854294753, 466.8603241254305, 401.11448015293473, 304.51641888197383, 150.5679929230338, 1090.2143485368044, 2081.9435486791353, 971.4656528686749, 444.5139282528746, 2979.9276965680715, 2018.5360120813696, 2578.827602485899, 1196.465691639731, 339.00938859204746, 2946.997491270579, 310.79125758804815, 839.4299039527606, 793.375361328035, 824.2883198141641, 1731.6182595055707, 2022.9551781449843, 2184.611161709893, 489.36173725028425, 2723.9741181951754, 3578.8924378156635, 766.72942234308, 1375.3319828431788, 6254.942998560112, 952.9795193173893, 3375.7856499988825, 1491.5551238663365, 3038.2682045957754, 1365.9971518565271, 2569.5041237727746, 2719.8815583457877, 2834.8498164435673, 1585.0656277097942, 1395.6662534760485, 2970.6650242855617, 1659.0444797073224, 1597.8593790027705, 2575.574327962084, 3041.1448968046648, 3526.8985031189923, 2211.474555451544, 1629.2455446147064, 1852.4963546183553, 2027.214368492002, 1717.7760539851415, 1665.6766866791634], \"Total\": [33239.0, 19186.0, 23120.0, 20753.0, 12982.0, 21656.0, 15353.0, 9333.0, 10461.0, 9873.0, 15464.0, 14957.0, 8766.0, 18208.0, 9440.0, 5678.0, 7774.0, 7261.0, 15091.0, 9078.0, 9889.0, 41501.0, 5509.0, 3754.0, 18720.0, 21151.0, 7507.0, 9126.0, 13684.0, 6287.0, 3.340521958704485, 11.733073664970199, 3.2333174106684948, 4.254456994995992, 13.87991087337049, 5.305271466755255, 56.0256385685038, 14.86033784213594, 6.323196099267982, 6.350173244244143, 2.1247869550958534, 2.11164542979202, 9.467901727682824, 7.44843478918532, 4.205363472654971, 5.297852241774547, 2.1049938626508524, 41.09379512059098, 9.42029640548712, 5.2858276012768215, 4.208559439841919, 37.800775974885866, 8.392323627390054, 3.1618407376347326, 21.948902191289186, 2.0935603367568634, 7.36070130477874, 9.42622342944885, 7.32277423215235, 6.272937380625953, 3592.3508246304878, 19.94698493807019, 261.84186234286193, 480.4332634671669, 16.65685036975742, 2555.5771380425626, 15091.870738798525, 3204.2433276231795, 633.1685017397074, 441.2273865093146, 188.54273661741962, 484.3575735435169, 15809.901458435335, 2942.560524273068, 19611.581330883906, 788.3726170634689, 3178.5699272122856, 976.1776632070796, 2724.2566127482887, 8764.273856130958, 7774.8370116349415, 3868.697532160153, 4617.57229865922, 1498.2038126439272, 5855.656418800656, 1696.1110798469708, 4347.5553569717, 3108.64263854364, 13356.437790631193, 5457.219865054934, 41501.6636461691, 3105.7221958045307, 17742.466323181005, 15799.118506936062, 8696.478174822048, 6695.539602472614, 5070.272610323575, 9345.64566905632, 3278.123189140766, 11513.556534329928, 7363.509376103886, 33239.769583169975, 12364.485135517049, 21151.668470523222, 11906.854797114007, 14957.729870801984, 9440.968724466074, 9998.954774646156, 10876.052262441865, 16498.33306037795, 19186.382067170453, 13092.188399663117, 18720.135201978395, 18208.19951648111, 13684.073691009993, 20753.55707550573, 21656.286676177453, 23120.317773523428, 12787.606724730762, 15464.643224970328, 15353.600023600673, 7.398416265786616, 12.036730743198689, 10.457575526864469, 8.695441383200453, 2.8517769909799084, 22.81277963002317, 3.873864420424712, 3.834176188805721, 3.833131313337823, 2.86779483384967, 1.899069894332433, 1.9182931873660305, 14.38709153555517, 22.081994000300487, 4.893044960094232, 15.344696681163827, 1.9228445728852988, 1.9390826954130231, 9.702512844611627, 3.7450334914661187, 7.753396666444077, 70.8894557297278, 90.36105954721623, 5.712102320554174, 2.8830318801483656, 12.72688437777213, 10.536109180741056, 2.890133912056269, 1.9371419931549434, 3.8801317209873147, 2040.771142872943, 77.54083977120777, 86.1286789764733, 6287.063757029713, 4082.1193894096778, 991.6787759903079, 655.8358187040699, 8529.408529025439, 8544.738061968841, 633.2882224570285, 3144.39591944302, 1259.4555952131184, 318.65715732075665, 2803.937887234506, 1821.8877428393473, 1400.8894205611177, 13684.073691009993, 4231.450824630156, 23120.317773523428, 16498.33306037795, 4776.182926646079, 2667.176347113611, 3878.2814647641108, 5977.066109239206, 6999.658658126856, 12787.606724730762, 1552.6099611752009, 5964.031892125886, 18208.19951648111, 18720.135201978395, 21656.286676177453, 4948.143670186049, 41501.6636461691, 3049.5099866076253, 33239.769583169975, 15799.118506936062, 20753.55707550573, 19611.581330883906, 15464.643224970328, 21151.668470523222, 12364.485135517049, 5372.443217700795, 17742.466323181005, 14957.729870801984, 19186.382067170453, 13356.437790631193, 12982.024530345443, 15809.901458435335, 15353.600023600673, 13092.188399663117, 9889.166855115516, 11513.556534329928, 9998.954774646156, 15091.870738798525, 11906.854797114007, 2.8581412012329164, 2.8790570113240346, 2.849474735289291, 2.8797992207215843, 2.9189048238521833, 6.73812842204899, 6.716631964954242, 9.603367899796917, 4.864609782559222, 4.814704190605031, 27.288581892339927, 12.29223192151307, 1.94757398071032, 1.9467286463477602, 1.9249188947354923, 1.9465196698497982, 3.0365501617673134, 2.8952171671765896, 2.8998803039874295, 1.9494591429080832, 4.790154696471541, 1.946226424038329, 1.9447124825238138, 6.776071658898481, 4.835054863367829, 1.9355151342715675, 2.8714664396468947, 1.9511877360160645, 1.9460902177561668, 7.714731643679967, 27.481744704036895, 1440.2943991085135, 47.65724809076298, 10.515228986625516, 1393.8937628130157, 1216.4206414791354, 2116.29123117264, 938.2172986739131, 3539.966008973561, 2668.0568967066893, 4464.427698797724, 2176.892881822283, 3238.2768920669887, 1062.0081163127134, 4586.461212523806, 1589.8537553967155, 4181.209763453312, 2220.5460085321693, 7507.556830856354, 2063.489755804687, 2201.433113963308, 7261.275598599436, 5509.375764666298, 9333.299979293202, 2126.897542785379, 8766.015518811579, 4455.387721462994, 20753.55707550573, 9078.290188847724, 15353.600023600673, 2247.185295074968, 3192.437060286453, 23120.317773523428, 18208.19951648111, 41501.6636461691, 33239.769583169975, 21151.668470523222, 21656.286676177453, 15091.870738798525, 18720.135201978395, 15464.643224970328, 13092.188399663117, 12787.606724730762, 12982.024530345443, 10461.36691528274, 7430.892978831589, 15809.901458435335, 8322.04351235751, 14957.729870801984, 13356.437790631193, 13684.073691009993, 9755.89821769538, 12364.485135517049, 17742.466323181005, 11906.854797114007, 16498.33306037795, 19186.382067170453, 19611.581330883906, 15799.118506936062, 2.9183437817909508, 2.9410674149388067, 2.9841293771117083, 4.895919087908704, 3.9234923201106464, 1.9607422942544328, 1.9451971363531064, 1.9587396049310797, 1.9578300194566247, 6.784204538081587, 4.831604739891672, 3.9275830709375565, 1.9503518186717614, 17.364309483102403, 1.9471573488334644, 7.688372335129623, 1.9632230498471068, 1.9505113944541965, 1.954001685874859, 1.9678134084559624, 1.955522306005622, 4.788383783651528, 5.820192995353047, 9.764668059067102, 1.9403750021787252, 1.9738897395292332, 1.950808398970051, 6.879467187507393, 4.831692493444994, 1.964740316748035, 3.886706341711976, 23.28858504091635, 13.459826670142817, 5.833195998748151, 201.0855949138522, 38.16172246997407, 8.68724970745323, 89.09245650836118, 2073.990321295984, 2287.3514766041753, 33239.769583169975, 60.24660138648288, 59.20366592684245, 20.184012886319668, 697.0520931134303, 1503.0489512778445, 1087.0541034430341, 114.1086539067096, 639.5184731858749, 75.54496986072105, 1037.3933072658915, 1933.437678181946, 1520.5804907464308, 745.4913997750753, 3359.087229912439, 2473.0367569676323, 2638.8012091400165, 513.5414780058064, 2135.6098440306273, 4948.458130913132, 11513.556534329928, 3943.0685983489825, 3904.052470286212, 2265.755675099432, 6287.063757029713, 998.9011224313233, 1306.0767528529732, 3683.0786825414407, 3004.9463182683107, 21656.286676177453, 20753.55707550573, 9333.299979293202, 4261.171119612729, 2592.6102422695712, 19186.382067170453, 23120.317773523428, 3754.3240207717636, 9873.382430644077, 3281.211207820451, 12982.024530345443, 13684.073691009993, 15799.118506936062, 15353.600023600673, 8764.273856130958, 9126.128780614414, 13092.188399663117, 7447.725950927597, 5855.656418800656, 18720.135201978395, 41501.6636461691, 8766.015518811579, 16498.33306037795, 15091.870738798525, 18208.19951648111, 7774.8370116349415, 17742.466323181005, 9345.64566905632, 12787.606724730762, 21151.668470523222, 19611.581330883906, 15464.643224970328, 13356.437790631193, 11906.854797114007, 10876.052262441865, 12364.485135517049, 10461.36691528274, 15809.901458435335, 1.9820470054190233, 3.9484834143391616, 1.9717384767246542, 1.9713069712867157, 48.29530302674425, 1.9898164319885674, 5.973334363880284, 8.99142018027843, 2.01485769280069, 5.004143260265599, 11.895391721448878, 3.8710499758613572, 3.9454354381972467, 1.9994008383581807, 1.9665108567104235, 4.888351862425173, 3.976658342350786, 1.9701816963120364, 1.9697210183251865, 2.0288341005242256, 4.906248225248758, 2.991859835103268, 4.963036384504894, 1.9834047617555273, 5.938708244347619, 3.032853405461486, 2.0158328899863696, 6.8184634080465765, 2.007989515568026, 3.006382676105114, 4.881081265106205, 52.5558072381467, 8.99740443199703, 20.332692151618346, 38.74069754354388, 14.864838044193878, 4007.5166562820154, 334.4216370622838, 1918.8858033605861, 10461.36691528274, 19186.382067170453, 9440.968724466074, 1971.117463847541, 1691.6958478080521, 1262.032820295495, 597.2701652483639, 4940.238516071662, 9889.166855115516, 4436.715020734303, 1933.3399487608356, 14957.729870801984, 9873.382430644077, 12982.024530345443, 5678.951027788038, 1454.7758996899188, 15464.643224970328, 1326.2060305244966, 3994.4903129102913, 3754.3240207717636, 3935.519270629899, 9126.128780614414, 10876.052262441865, 11906.854797114007, 2217.1823040519494, 15353.600023600673, 21151.668470523222, 3702.0028666373782, 7261.275598599436, 41501.6636461691, 4776.182926646079, 21656.286676177453, 8288.004016551498, 19611.581330883906, 7533.567114873256, 16498.33306037795, 17742.466323181005, 18720.135201978395, 9078.290188847724, 7774.8370116349415, 20753.55707550573, 9755.89821769538, 9333.299979293202, 18208.19951648111, 23120.317773523428, 33239.769583169975, 15809.901458435335, 9998.954774646156, 13092.188399663117, 15799.118506936062, 11513.556534329928, 12364.485135517049], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -12.9814, -11.9325, -13.2258, -12.9752, -11.8007, -12.7637, -10.4178, -11.7573, -12.6173, -12.6166, -13.7215, -13.7279, -12.2368, -12.479, -13.054, -12.8324, -13.7602, -10.7943, -12.2704, -12.8508, -13.0812, -10.8883, -12.4018, -13.3793, -11.4424, -13.7981, -12.5414, -12.2962, -12.5492, -12.7043, -6.3937, -11.5562, -9.0218, -8.427, -11.7359, -6.8085, -5.0952, -6.6013, -8.1846, -8.5379, -9.3661, -8.4527, -5.0849, -6.711, -4.8946, -7.9929, -6.6601, -7.7968, -6.8127, -5.6914, -5.8074, -6.4813, -6.3161, -7.388, -6.0935, -7.2758, -6.3852, -6.702, -5.3316, -6.1894, -4.306, -6.7173, -5.0977, -5.2065, -5.7687, -6.0138, -6.275, -5.7198, -6.6859, -5.5511, -5.9544, -4.6109, -5.4924, -5.0245, -5.5327, -5.3586, -5.764, -5.7184, -5.6538, -5.348, -5.2308, -5.5495, -5.2986, -5.3389, -5.5604, -5.2814, -5.2574, -5.2353, -5.6169, -5.5355, -5.5864, -12.0228, -11.5881, -11.953, -12.1722, -13.3419, -11.2801, -13.0801, -13.1317, -13.1332, -13.4358, -13.8484, -13.8437, -11.8301, -11.4185, -12.9266, -11.8055, -13.8868, -13.8881, -12.2862, -13.2406, -12.5282, -10.3193, -10.0778, -12.8398, -13.5251, -12.0428, -12.2364, -13.5319, -13.9404, -13.2508, -7.0206, -10.2635, -10.1603, -5.9411, -6.3678, -7.7637, -8.1695, -5.6719, -5.676, -8.2139, -6.6602, -7.5518, -8.892, -6.7892, -7.2114, -7.4668, -5.2795, -6.4072, -4.787, -5.1159, -6.3013, -6.8614, -6.5095, -6.1015, -5.9589, -5.3972, -7.3856, -6.1234, -5.0793, -5.0538, -4.9265, -6.2997, -4.3655, -6.7559, -4.5868, -5.2678, -5.0267, -5.0827, -5.2951, -5.0383, -5.5222, -6.2517, -5.2325, -5.3791, -5.1884, -5.4951, -5.5249, -5.3874, -5.4174, -5.5555, -5.7776, -5.6806, -5.7902, -5.5848, -5.774, -13.1458, -13.2032, -13.2312, -13.2429, -13.2457, -12.4096, -12.4165, -12.0754, -12.7744, -12.8264, -11.0996, -11.8974, -13.7501, -13.7582, -13.7708, -13.7613, -13.3203, -13.3698, -13.3718, -13.771, -12.8759, -13.7788, -13.788, -12.5456, -12.8839, -13.7999, -13.4097, -13.7982, -13.8037, -12.4273, -11.1594, -7.3114, -10.6521, -12.136, -7.3496, -7.5102, -6.9801, -7.7713, -6.5133, -6.7862, -6.3083, -6.9987, -6.6213, -7.7007, -6.2968, -7.3146, -6.4008, -7.0067, -5.8554, -7.0836, -7.0268, -5.9052, -6.1783, -5.6897, -7.0667, -5.7497, -6.3798, -4.9608, -5.7299, -5.2533, -7.0234, -6.704, -4.8862, -5.1118, -4.3864, -4.6441, -5.0633, -5.0521, -5.3656, -5.187, -5.3539, -5.5039, -5.5372, -5.5303, -5.7117, -5.9983, -5.3975, -5.913, -5.4735, -5.5775, -5.5615, -5.8082, -5.6482, -5.4088, -5.7264, -5.5789, -5.5139, -5.5198, -5.6755, -13.0576, -13.1674, -13.1746, -12.6814, -12.9052, -13.6125, -13.6247, -13.6318, -13.6373, -12.3983, -12.7412, -12.9492, -13.6542, -11.49, -13.6798, -12.3097, -13.6762, -13.6893, -13.6906, -13.6959, -13.7043, -12.8091, -12.6209, -12.1038, -13.7233, -13.7078, -13.724, -12.4647, -12.8191, -13.719, -13.0408, -11.2801, -11.8206, -12.6423, -9.1756, -10.8062, -12.2558, -10.001, -7.0279, -6.9498, -4.4091, -10.4068, -10.4272, -11.4512, -8.0918, -7.3703, -7.6808, -9.8079, -8.1899, -10.1979, -7.7361, -7.1603, -7.3841, -8.0477, -6.649, -6.9366, -6.8793, -8.403, -7.0785, -6.2979, -5.5242, -6.5137, -6.5275, -7.0283, -6.0894, -7.7883, -7.5422, -6.5889, -6.7772, -4.9646, -5.0116, -5.74, -6.4614, -6.9141, -5.0933, -4.9334, -6.5807, -5.7232, -6.7091, -5.5035, -5.4582, -5.3589, -5.3881, -5.8711, -5.8424, -5.5396, -6.0178, -6.2246, -5.2653, -4.6291, -5.9029, -5.3972, -5.4783, -5.3375, -6.014, -5.389, -5.8916, -5.6847, -5.3473, -5.4301, -5.5874, -5.7157, -5.7813, -5.8389, -5.8132, -5.8664, -5.8168, -13.1854, -12.6011, -13.3049, -13.3108, -10.1531, -13.382, -12.3255, -11.9786, -13.4843, -12.5759, -11.7362, -12.8796, -12.871, -13.5862, -13.6123, -12.736, -12.9426, -13.6449, -13.6557, -13.6311, -12.7512, -13.2579, -12.7571, -13.6748, -12.5828, -13.2564, -13.6719, -12.458, -13.681, -13.2778, -12.7982, -10.4585, -12.1992, -11.4099, -10.7983, -11.7239, -6.3761, -8.7545, -7.1129, -5.5065, -4.9371, -5.6063, -7.0904, -7.2422, -7.5177, -8.222, -6.2423, -5.5954, -6.3577, -7.1395, -5.2368, -5.6263, -5.3814, -6.1493, -7.4104, -5.2479, -7.4973, -6.5037, -6.5602, -6.5219, -5.7797, -5.6241, -5.5473, -7.0434, -5.3266, -5.0537, -6.5943, -6.01, -4.4953, -6.3769, -5.1121, -5.9289, -5.2174, -6.0168, -5.385, -5.3281, -5.2867, -5.8681, -5.9953, -5.2399, -5.8225, -5.86, -5.3826, -5.2165, -5.0683, -5.535, -5.8406, -5.7122, -5.622, -5.7877, -5.8185], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8582, 0.6508, 0.6464, 0.6226, 0.6146, 0.6133, 0.6022, 0.5898, 0.5843, 0.5807, 0.5706, 0.5704, 0.561, 0.5587, 0.5554, 0.5461, 0.5413, 0.5356, 0.5325, 0.53, 0.5275, 0.5252, 0.5167, 0.5154, 0.5147, 0.5089, 0.5083, 0.5061, 0.5056, 0.5052, 0.4655, 0.4966, 0.4562, 0.4441, 0.497, 0.3913, 0.3287, 0.3722, 0.4105, 0.4183, 0.4403, 0.4102, 0.2924, 0.3477, 0.2673, 0.3829, 0.3215, 0.3654, 0.3232, 0.276, 0.2798, 0.3038, 0.292, 0.3458, 0.2771, 0.3339, 0.2832, 0.3019, 0.2144, 0.2517, 0.1063, 0.2875, 0.1644, 0.1716, 0.2064, 0.2228, 0.2396, 0.1833, 0.2648, 0.1434, 0.1871, 0.0233, 0.1308, 0.0618, 0.1282, 0.0741, 0.129, 0.1172, 0.0977, -0.0132, -0.047, 0.0165, -0.0902, -0.1027, -0.0386, -0.1761, -0.1947, -0.238, -0.0274, -0.136, -0.1797, 1.0217, 0.9697, 0.7454, 0.7107, 0.656, 0.6384, 0.6115, 0.5701, 0.5689, 0.5564, 0.556, 0.5506, 0.5494, 0.5325, 0.5313, 0.5095, 0.5052, 0.4955, 0.4872, 0.4847, 0.4695, 0.4653, 0.4641, 0.4634, 0.4618, 0.4593, 0.4546, 0.4526, 0.4442, 0.4391, 0.4041, 0.4314, 0.4296, 0.3584, 0.3636, 0.3826, 0.3904, 0.3226, 0.3167, 0.3809, 0.3322, 0.3556, 0.3897, 0.3178, 0.3268, 0.3341, 0.2423, 0.2883, 0.2103, 0.2188, 0.2731, 0.2956, 0.2732, 0.2485, 0.2332, 0.1923, 0.3125, 0.2289, 0.1569, 0.1546, 0.1363, 0.2393, 0.0468, 0.2671, 0.0475, 0.1103, 0.0786, 0.0792, 0.1044, 0.048, 0.101, 0.2051, 0.0296, 0.0537, -0.0046, 0.0509, 0.0496, -0.0101, -0.0108, 0.0105, 0.0689, 0.0139, 0.0453, -0.1609, -0.1131, 0.8499, 0.7851, 0.7675, 0.7451, 0.7289, 0.7284, 0.7247, 0.7083, 0.6894, 0.6477, 0.6397, 0.6394, 0.6291, 0.6214, 0.6201, 0.6184, 0.6148, 0.6129, 0.6093, 0.6072, 0.6033, 0.6011, 0.5926, 0.5867, 0.586, 0.5855, 0.5813, 0.5791, 0.5762, 0.5754, 0.5729, 0.4618, 0.5297, 0.5569, 0.4563, 0.432, 0.4083, 0.4305, 0.3607, 0.3705, 0.3336, 0.3615, 0.3417, 0.3771, 0.3181, 0.3597, 0.3066, 0.3335, 0.2668, 0.33, 0.3221, 0.2502, 0.2533, 0.2147, 0.3166, 0.2175, 0.2642, 0.1445, 0.2022, 0.1534, 0.3049, 0.2733, 0.1111, 0.1243, 0.0259, -0.0098, 0.023, 0.0107, 0.0583, 0.0214, 0.0456, 0.0621, 0.0523, 0.0442, 0.0786, 0.1341, -0.0201, 0.1061, -0.0407, -0.0314, -0.0397, 0.0519, -0.025, -0.1467, -0.0655, -0.2441, -0.3301, -0.3579, -0.2975, 0.9172, 0.7996, 0.7779, 0.776, 0.7736, 0.76, 0.7558, 0.7417, 0.7367, 0.7329, 0.7294, 0.7286, 0.7236, 0.7013, 0.6996, 0.6964, 0.695, 0.6884, 0.6853, 0.673, 0.6708, 0.6705, 0.6636, 0.6632, 0.6596, 0.6579, 0.6536, 0.6525, 0.6515, 0.6514, 0.6474, 0.6177, 0.6254, 0.6399, 0.5664, 0.5978, 0.6281, 0.5551, 0.3806, 0.3608, 0.2251, 0.5405, 0.5376, 0.5897, 0.4071, 0.3602, 0.3738, 0.5008, 0.3952, 0.5232, 0.3652, 0.3185, 0.3348, 0.3841, 0.2773, 0.296, 0.2884, 0.4014, 0.3007, 0.2411, 0.1703, 0.2523, 0.2485, 0.2918, 0.2101, 0.3508, 0.3288, 0.2453, 0.2606, 0.0982, 0.0937, 0.1644, 0.227, 0.2712, 0.0906, 0.0639, 0.2344, 0.1249, 0.2407, 0.071, 0.0636, 0.0191, 0.0186, 0.0963, 0.0845, 0.0264, 0.1123, 0.146, -0.0569, -0.2169, 0.0642, -0.0624, -0.0544, -0.1014, 0.0731, -0.127, 0.0115, -0.0951, -0.261, -0.2682, -0.1879, -0.1697, -0.1203, -0.0874, -0.19, -0.0761, -0.4394, 1.1763, 1.0713, 1.062, 1.0562, 1.0154, 0.9757, 0.933, 0.8709, 0.861, 0.8596, 0.8335, 0.8127, 0.8022, 0.7668, 0.7573, 0.7229, 0.7228, 0.7227, 0.7122, 0.7072, 0.704, 0.692, 0.6867, 0.6862, 0.6815, 0.6799, 0.6728, 0.6682, 0.6676, 0.6672, 0.6622, 0.6254, 0.6497, 0.6237, 0.5905, 0.6229, 0.3737, 0.4789, 0.3734, 0.2838, 0.2467, 0.2866, 0.369, 0.3701, 0.3876, 0.4314, 0.2983, 0.2512, 0.2905, 0.3393, 0.196, 0.2218, 0.1931, 0.2519, 0.3527, 0.1515, 0.3584, 0.2494, 0.2549, 0.246, 0.1472, 0.1273, 0.1136, 0.2984, 0.0801, 0.0327, 0.2348, 0.1455, -0.083, 0.1975, -0.0494, 0.0943, -0.0555, 0.1018, -0.0502, -0.0661, -0.0783, 0.0641, 0.0918, -0.1346, 0.0377, 0.0444, -0.1465, -0.2192, -0.434, -0.1577, -0.005, -0.1462, -0.244, -0.0932, -0.1953]}, \"token.table\": {\"Topic\": [1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 2, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 4, 1, 2, 5, 1, 2, 3, 4, 5, 2, 3, 4, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 5, 1, 5, 1, 2, 1, 5, 5, 3, 1, 2, 3, 4, 1, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 4, 1, 3, 4, 1, 3, 4, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1], \"Freq\": [0.5987088319502131, 0.20487263900906214, 0.20487263900906214, 0.20487263900906214, 0.20487263900906214, 0.40974527801812427, 0.40968079922876766, 0.2731205328191784, 0.1365602664095892, 0.1365602664095892, 0.1365602664095892, 0.2192393073141876, 0.37399646541832, 0.1805500177881545, 0.12896429842011034, 0.10317143873608828, 0.2085202552987188, 0.4170405105974376, 0.13901350353247918, 0.13901350353247918, 0.13901350353247918, 0.3486999795789506, 0.23783392625852615, 0.18701471124602057, 0.19514578564802146, 0.17049846636695623, 0.20937516585152302, 0.47009524419975585, 0.23504762209987792, 0.23504762209987792, 0.19124891757771192, 0.4781222939442798, 0.09562445878885596, 0.09562445878885596, 0.09562445878885596, 0.506613910581701, 0.10993609018730718, 0.2931629071661525, 0.3298082705619216, 0.07329072679153813, 0.1832268169788453, 0.35127871774659447, 0.27021439826661114, 0.11451943545584949, 0.0813860032874436, 0.18271640263742278, 0.20696681366967554, 0.20696681366967554, 0.20696681366967554, 0.4139336273393511, 0.20696681366967554, 0.20883873247883641, 0.20883873247883641, 0.20883873247883641, 0.41767746495767283, 0.20883873247883641, 0.20769707969833487, 0.20769707969833487, 0.41539415939666974, 0.20769707969833487, 0.20769707969833487, 0.17175796610108734, 0.21469745762635917, 0.1288184745758155, 0.30057644067690287, 0.1288184745758155, 0.2043717170301132, 0.4087434340602264, 0.2043717170301132, 0.2043717170301132, 0.2043717170301132, 0.17126918093442042, 0.20882587535394342, 0.2008350893072364, 0.2080267967492727, 0.21122311116795553, 0.17143260747874883, 0.17143260747874883, 0.17143260747874883, 0.34286521495749767, 0.17143260747874883, 0.21023101815008519, 0.1611771139150653, 0.1972167170265085, 0.23325632013795167, 0.19821781711293746, 0.40588363635093394, 0.1810865454488782, 0.11239854545102786, 0.12072436363258547, 0.1831679999942676, 0.19648760709974714, 0.2471716894965017, 0.2735551844427575, 0.13330607972845035, 0.14996933969450665, 0.3739073467434702, 0.1987343158033787, 0.14341651655913926, 0.1311236722826416, 0.15366055345622065, 0.19272802898743432, 0.2520408116894025, 0.17994111739194507, 0.17662884511118582, 0.19865930725763115, 0.229695536150295, 0.19830381287642135, 0.22892988436312733, 0.22892988436312733, 0.1148477680751475, 0.2073689727235937, 0.35374707111671866, 0.14485332653486324, 0.14027901096007808, 0.15247718582617184, 0.5126080042140274, 0.21774580538453933, 0.25947359194417885, 0.19895384607938987, 0.18069191639566773, 0.1431561822964077, 0.4980105684053421, 0.2422892081913837, 0.22004495433324125, 0.2248401228296073, 0.16183693675235358, 0.15091460851063096, 0.17276817157165777, 0.2303575620955437, 0.17276817157165777, 0.34553634314331555, 0.11517878104777185, 0.25547651173534786, 0.30750872859827727, 0.14187562668734446, 0.1816944936113869, 0.1136008850480034, 0.34001260730053906, 0.34001260730053906, 0.2997336218144382, 0.24319157956546708, 0.11282352209126491, 0.19507438846572825, 0.14921540489052587, 0.1624998867129489, 0.29986166940400516, 0.22422290998098887, 0.17506893872389523, 0.13825957212040957, 0.2524360588185185, 0.3292084531555366, 0.15670462807510258, 0.12124420812980304, 0.14043730671405757, 0.37751147233394494, 0.18875573616697247, 0.18875573616697247, 0.18875573616697247, 0.18875573616697247, 0.22228632880917276, 0.22228632880917276, 0.11114316440458638, 0.11114316440458638, 0.33342949321375914, 0.20662855032796754, 0.22765533040048086, 0.17751454715064138, 0.22118555191663059, 0.16659679595914406, 0.291930513317408, 0.25092622744548704, 0.1518158681794537, 0.14231487511156957, 0.16291702850087622, 0.26174512343815587, 0.2692812298199228, 0.1968807792236617, 0.15785451403236855, 0.11425275568071688, 0.2581401648254806, 0.5162803296509612, 0.5157077634520382, 0.2579514612809376, 0.3869271919214064, 0.1289757306404688, 0.1289757306404688, 0.1289757306404688, 0.358022641017706, 0.21582032665917952, 0.13276410765331453, 0.149752879722696, 0.14346074191922137, 0.342574744235227, 0.18546170101667822, 0.17709372371482074, 0.19041417737900204, 0.10451432874973027, 0.2557117984903615, 0.32953654366121904, 0.15870537005329272, 0.14693620777967775, 0.1091322319917024, 0.3473359492593428, 0.3473359492593428, 0.26799260739937614, 0.21420023954886847, 0.23940026773108827, 0.14586939390092626, 0.13230014795665404, 0.23181918839351107, 0.25297728892149024, 0.12050918126805536, 0.2391785277075908, 0.15546604300993402, 0.23555436494787232, 0.22568082869257228, 0.23649470173409137, 0.17302196866430541, 0.12976647649822906, 0.1296221367361775, 0.1296221367361775, 0.259244273472355, 0.1296221367361775, 0.259244273472355, 0.25728725354628423, 0.25728725354628423, 0.25728725354628423, 0.25728725354628423, 0.25728725354628423, 0.2064052963824775, 0.21891470828444584, 0.17825911960304874, 0.2439335320883825, 0.15167661931136606, 0.5138148304065498, 0.24910803308135704, 0.17239347321630258, 0.2134293341497046, 0.19392926446333603, 0.17121489757591765, 0.39435448777685234, 0.2039764591949236, 0.1269186857212858, 0.13371790102778325, 0.1405171163342807, 0.5142148307199723, 0.514086711989932, 0.39168088639689097, 0.2100546689144617, 0.13740418192148998, 0.13424546509570862, 0.12792803144414586, 0.2468128988282965, 0.3014739076720632, 0.12541851108937893, 0.16180329304980035, 0.16448604195010794, 0.3460047286488955, 0.3460047286488955, 0.49607286644021453, 0.49607286644021453, 0.19023002358271138, 0.2944656529431012, 0.18185394622339435, 0.16398498119018468, 0.16938289771063342, 0.18884850385946642, 0.18884850385946642, 0.29376433933694773, 0.10491583547748133, 0.20983167095496266, 0.18493003997268928, 0.205477822191877, 0.20853813018196876, 0.23608090209279484, 0.16481944460922898, 0.2678906814312866, 0.22695057729228596, 0.23362559427147087, 0.12949532939618671, 0.14195536109066514, 0.2555163670603438, 0.22499923010374406, 0.1655166750188462, 0.12413750626413465, 0.230171626198083, 0.5045289023246906, 0.23430375787357982, 0.21049749370460646, 0.22615950960524683, 0.1509818332821731, 0.17792050063127451, 0.520062835083682, 0.24985249840083626, 0.3056107482633218, 0.16544780824202787, 0.17531329150734315, 0.10377026842035318, 0.22251746201799677, 0.21795975341953985, 0.16863521814290608, 0.18646092288353758, 0.2044891924507672, 0.2671286262775187, 0.315088012848579, 0.12892308218026965, 0.1482615445073101, 0.14052615957649392, 0.3478832562277286, 0.23744613525070785, 0.16875500502101454, 0.10600951589775875, 0.1398490690035469, 0.2698250288247143, 0.2469516392844347, 0.11861775460711371, 0.1437177572884825, 0.22063712034429, 0.2169523756565091, 0.2372733426948852, 0.20066954950396412, 0.16764797806660295, 0.17741767375812992, 0.2046684672050191, 0.29597344063481523, 0.19238489901266378, 0.17538686274648194, 0.1315293340596921, 0.2954146000687794, 0.24902105584858386, 0.09977789647357126, 0.1375918126530457, 0.2180920263684535, 0.22909824499562223, 0.1940696600641091, 0.24689502604953617, 0.180792696420713, 0.1491539745470882, 0.474443612518565, 0.15814787083952167, 0.15814787083952167, 0.15814787083952167, 0.15814787083952167, 0.19983440680851414, 0.19983440680851414, 0.19983440680851414, 0.19983440680851414, 0.3996688136170283, 0.23235550818265424, 0.25569252428833567, 0.11516310121716705, 0.1598078276802098, 0.2369214461163745, 0.20556633413541964, 0.20556633413541964, 0.4111326682708393, 0.20556633413541964, 0.20556633413541964, 0.45119600921856023, 0.10026577982634671, 0.15039866973952007, 0.15039866973952007, 0.15039866973952007, 0.1811430616250312, 0.40757188865632016, 0.13585729621877338, 0.13585729621877338, 0.1811430616250312, 0.24382122481799948, 0.3715371044845706, 0.12771587966657116, 0.10449481063628549, 0.1509369486968568, 0.2454206245677843, 0.18177519737142372, 0.1755070871172367, 0.24059900129533274, 0.15622059402743046, 0.471052547685138, 0.13458644219575372, 0.13458644219575372, 0.13458644219575372, 0.13458644219575372, 0.30830833997868645, 0.2677997508622095, 0.12785523439888044, 0.16773087681041246, 0.1282982970923419, 0.14757813233671738, 0.14757813233671738, 0.29515626467343475, 0.14757813233671738, 0.14757813233671738, 0.2634644731053725, 0.2428397289986388, 0.12907097924859157, 0.23618658573840212, 0.1284056649225679, 0.17506689199205075, 0.3501337839841015, 0.17506689199205075, 0.17506689199205075, 0.17506689199205075, 0.5126860590731519, 0.4782448505329478, 0.15941495017764928, 0.15941495017764928, 0.15941495017764928, 0.15941495017764928, 0.1822456566114876, 0.2781227194375311, 0.1735295599909382, 0.12519484236789147, 0.24167358811523354, 0.20456792557970394, 0.20456792557970394, 0.20456792557970394, 0.20456792557970394, 0.40913585115940787, 0.3478018092897702, 0.20530268692820808, 0.157658603463856, 0.10849857188927453, 0.18083095314879088, 0.24069664303376656, 0.3312070221969754, 0.1619104062492117, 0.1396345357297691, 0.12650349626567664, 0.251818316248735, 0.24649821097587443, 0.12827364935675, 0.1365493686700887, 0.2370402460463445, 0.4086435951936962, 0.2138695451481027, 0.137487564738066, 0.1565830598405752, 0.08020107943053852, 0.26250670682826205, 0.2570170228451615, 0.11578242582539314, 0.12701132488173514, 0.23805265999445055, 0.3060453885661635, 0.24703442690340596, 0.1486828241321435, 0.14490657348132713, 0.15330450403314266, 0.16838678696697826, 0.16838678696697826, 0.16838678696697826, 0.16838678696697826, 0.3367735739339565, 0.2170840570804067, 0.3131650672383511, 0.16232116050574216, 0.14166210371410226, 0.16592829740586978, 0.2796547361217803, 0.25304642032534985, 0.16526572022305544, 0.10275623462088844, 0.1992280931491526, 0.20552993767887615, 0.23783731918692694, 0.16428647192391774, 0.1594747342525059, 0.2330255815155151, 0.23728461103905688, 0.2799659267122236, 0.17590684920117386, 0.15544759525521284, 0.1514412139342022, 0.20211965967626278, 0.35055128475101827, 0.19580342031137957, 0.12474572745644344, 0.12632478729766425, 0.25345744865538117, 0.25345744865538117, 0.25345744865538117, 0.25345744865538117, 0.44972621755951475, 0.2380903504726843, 0.05290896677170762, 0.15872690031512285, 0.10581793354341525, 0.2863171236086771, 0.22692057195446858, 0.14996250115791665, 0.15079000729551398, 0.18600499070660048, 0.2712128202917643, 0.3047607548435026, 0.10549410744582784, 0.18208848813925457, 0.1366168901022598, 0.2417260730329367, 0.30287191183795853, 0.1925808194139472, 0.1107196847520839, 0.15200741235640935, 0.216773680325177, 0.25931130939120395, 0.19141933079712137, 0.1649835161233193, 0.16750693479672768, 0.21375733818469927, 0.2797495365504454, 0.12624594469968817, 0.24675343736757233, 0.13341900973944318, 0.24921017096721776, 0.164545679103115, 0.233903596166928, 0.18080891482842287, 0.1714814708094963, 0.18982339359730613, 0.37964678719461226, 0.18982339359730613, 0.18982339359730613, 0.09491169679865306, 0.20027586433602812, 0.2520972442329754, 0.15446276036916168, 0.1830020710370457, 0.2100393127224095, 0.23000557554950452, 0.46001115109900903, 0.11500278777475226, 0.11500278777475226, 0.11500278777475226, 0.511372331028335, 0.20485041392534575, 0.3422113503946667, 0.1722966659759691, 0.1564167889274927, 0.12465703483053986, 0.21981572242847613, 0.23740098022275422, 0.16809437597471705, 0.22602228400292723, 0.14844026432228857, 0.20181854595932064, 0.20181854595932064, 0.13454569730621377, 0.13454569730621377, 0.3363642432655344, 0.19550728582876514, 0.3910145716575303, 0.13033819055251009, 0.19550728582876514, 0.13033819055251009, 0.16446304277183063, 0.23323849702186888, 0.14652161992399454, 0.19137517704358473, 0.263140868434929, 0.23434497167815715, 0.28058787445598893, 0.1949671079112861, 0.14866928482136663, 0.14147472393787972, 0.25326179574882807, 0.25326179574882807, 0.5065235914976561, 0.26788547178389016, 0.22770265101630663, 0.14566272528249027, 0.10715418871355606, 0.25281691399604633, 0.2066206197568622, 0.2115078354672742, 0.17756883747830207, 0.21042178753162707, 0.19413106849692047, 0.4243480997388028, 0.2121740498694014, 0.1060870249347007, 0.1060870249347007, 0.1060870249347007, 0.5105323839281771, 0.5212967478517087, 0.2371484151258687, 0.20671299134955223, 0.22117326056454423, 0.1455667100975861, 0.18936066829156184, 0.3509418727653592, 0.3509418727653592, 0.2923334029656335, 0.19969729763111124, 0.12058471132352655, 0.1683678118853726, 0.2188556162526061, 0.2048200704726338, 0.2048200704726338, 0.1024100352363169, 0.3072301057089507, 0.1024100352363169, 0.21435754525566658, 0.25280090768846597, 0.21072249952419844, 0.14760488727779714, 0.17459234801142423, 0.20282875340392348, 0.2542774416970447, 0.2140082909931161, 0.17545029400181908, 0.1534334495659601, 0.2526665129411092, 0.2906720608486855, 0.18142566079336842, 0.14959796943866974, 0.1255903496698921, 0.3217923871149844, 0.2523876540168945, 0.1668109442745894, 0.1388843364584216, 0.12016677089798743, 0.31178722194253755, 0.2160707251364019, 0.23656582923612116, 0.11970012926325393, 0.11577553486118003, 0.4755831482830996, 0.2377915741415498, 0.2377915741415498, 0.2377915741415498, 0.5127280065198827, 0.3625941763528173, 0.19574189682786236, 0.11496888518503964, 0.1686210316047248, 0.15800851912610578, 0.21958099716433116, 0.1857993052928956, 0.13512676748574223, 0.2871443809072023, 0.1689084593571778, 0.25146741658697636, 0.25146741658697636, 0.25146741658697636, 0.25146741658697636, 0.25146741658697636, 0.5162244190325672, 0.27689406202828987, 0.24941703861512782, 0.1616763986339679, 0.13074815488871788, 0.18132180667816103, 0.1981813667916731, 0.34491886828513685, 0.16780498918701617, 0.12885463403265768, 0.16021089478585193, 0.43802233274338515, 0.1946765923303934, 0.04866914808259835, 0.17034201828909423, 0.12167287020649588, 0.3435441792545469, 0.1467554880304897, 0.15318649101166804, 0.17698120204202789, 0.17955360323449923, 0.47662604274997056, 0.23831302137498528, 0.11915651068749264, 0.11915651068749264, 0.11915651068749264, 0.13150523735616138, 0.43835079118720455, 0.08767015823744091, 0.08767015823744091, 0.21917539559360227, 0.29142705966327476, 0.14404068568251283, 0.17696930209159584, 0.17696930209159584, 0.21060227393188918, 0.08135219107360517, 0.08135219107360517, 0.32540876429442067, 0.32540876429442067, 0.16270438214721034, 0.2138392857947969, 0.1541632060381094, 0.19394725920923442, 0.2884343854906563, 0.1491901993917188, 0.21516144534189682, 0.33262203029895276, 0.1344759033386855, 0.15643115286336887, 0.1613710840064226, 0.5136822750700725, 0.5071666510566585, 0.5265735626605377, 0.21705386956613243, 0.3278350730242178, 0.10562858934375585, 0.19386710605164945, 0.1552225001941778, 0.14859032356163163, 0.22288548534244743, 0.22288548534244743, 0.29718064712326325, 0.14859032356163163, 0.511770285168548, 0.2608833139945883, 0.5217666279891766, 0.2608833139945883, 0.1475456362408566, 0.19672751498780877, 0.1475456362408566, 0.19672751498780877, 0.2950912724817132, 0.2691773150859216, 0.12423568388580997, 0.10352973657150831, 0.04141189462860332, 0.4555308409146365, 0.23104055326355746, 0.21268731113422235, 0.18620893577054612, 0.1524657353973414, 0.21756239107482697, 0.2073530231891064, 0.334245440754219, 0.17109804674193135, 0.10844690323233938, 0.17873067336238924, 0.26645486942728375, 0.1584571532765795, 0.22180371283388334, 0.18568346827255536, 0.1677141003752113, 0.22458207559025156, 0.32243837229986033, 0.17696617642503962, 0.14172291247598512, 0.13422434567831396, 0.5654753048546304, 0.1884917682848768, 0.1884917682848768, 0.1884917682848768, 0.34218465205786336, 0.1696660812304244, 0.17000837998206614, 0.1811901392023631, 0.13703360024057815, 0.2489287530582379, 0.2575594944963885, 0.23757251432382923, 0.09448390627028007, 0.16171283957797936, 0.23217324913597892, 0.2569478336474401, 0.15774837484848742, 0.1425802618822867, 0.21053340797086592, 0.2369922958423224, 0.3347873094881, 0.14990476544243284, 0.15632925538996567, 0.12135147678673135, 0.5001498352982364, 0.22664603049752705, 0.2662201733404619, 0.18021754265238454, 0.136375600091094, 0.19056372378779235, 0.2888076994259577, 0.2513393219349378, 0.17669171198819852, 0.13245252158722587, 0.15071684965037252, 0.3392893152130176, 0.2595915094303385, 0.12038804827441155, 0.1258440081072629, 0.15490846702992897, 0.5129627895192825, 0.29600909054325525, 0.26535678307989635, 0.16790023824256775, 0.1360347787687856, 0.13474074995766758, 0.21599908200097198, 0.3432444911326197, 0.11038539242170439, 0.2029564275649781, 0.12740446589306226, 0.29520810154264826, 0.2141623496255343, 0.16125165148275522, 0.1458823534508051, 0.18350773879678134, 0.3607240012998813, 0.20421590227888212, 0.18248234746140213, 0.1558454906424175, 0.09674082327292922, 0.403091635156516, 0.169722793750112, 0.148507444531348, 0.10607674609382, 0.169722793750112, 0.25487497321564756, 0.25487497321564756, 0.25487497321564756, 0.25487497321564756, 0.25487497321564756, 0.3586299452951979, 0.18059972680167594, 0.14646197356477378, 0.1527022080274333, 0.16151195079824676, 0.5043260049623196, 0.14409314427494846, 0.14409314427494846, 0.07204657213747423, 0.14409314427494846, 0.41365670351888617, 0.22742767616078735, 0.1255445311487333, 0.11218280721272621, 0.12109062317006426, 0.20425174151053035, 0.20425174151053035, 0.20425174151053035, 0.4085034830210607, 0.20425174151053035, 0.24777990886226028, 0.23876309686537955, 0.12378571383001014, 0.18012775873537476, 0.20957572855177717, 0.18086018423796893, 0.21243178747152958, 0.21964815392491488, 0.16597642842786176, 0.22055019973158804, 0.508973115416679, 0.5135691784739927, 0.25772320939288784, 0.25772320939288784, 0.25772320939288784, 0.25772320939288784, 0.26164863679798395, 0.21640679758219422, 0.1485440387585096, 0.13949567091535167, 0.2345035332685101, 0.3805307205080769, 0.21436563921954999, 0.1496754167331769, 0.13952793085296153, 0.11669608762247692, 0.4246150893585563, 0.21230754467927815, 0.10615377233963907, 0.10615377233963907, 0.10615377233963907, 0.21179437928823716, 0.15884578446617786, 0.19855723058272234, 0.2779801228158113, 0.15884578446617786, 0.17199380794157623, 0.3591779521970809, 0.18767415510434102, 0.13426297258117348, 0.14651324380208347, 0.42248009274373416, 0.21124004637186708, 0.10562002318593354, 0.10562002318593354, 0.10562002318593354, 0.47063541951898324, 0.39227217737089753, 0.16929641339165052, 0.1775547750205115, 0.11355247239683876, 0.14658591891228276, 0.21374855575272908, 0.2748393613826406, 0.17399104732691362, 0.18151773010671376, 0.1558900678810139, 0.27964095342203654, 0.2194699344788121, 0.1604560505152651, 0.21561281787988748, 0.12458486614526593, 0.1290632414240886, 0.20650118627854175, 0.18068853799372403, 0.18068853799372403, 0.2839391311329949, 0.5085148635654108, 0.34825411371444426, 0.34825411371444426, 0.34825411371444426, 0.2093013232611114, 0.190273930237374, 0.1331917511661618, 0.15221914418989918, 0.30443828837979836, 0.25626831417010704, 0.2985150064540627, 0.1348621095147796, 0.15456106933154515, 0.15577331301257688, 0.14536009442939513, 0.14536009442939513, 0.14536009442939513, 0.29072018885879025, 0.14536009442939513, 0.24519212731471848, 0.17787237447516102, 0.2424128714635441, 0.16675535107046346, 0.16768176968752158, 0.2821735933442077, 0.16598446667306335, 0.11618912667114435, 0.2821735933442077, 0.16598446667306335, 0.34484181937611963, 0.34484181937611963, 0.34484181937611963, 0.316271464307866, 0.3293210869989338, 0.3293210869989338, 0.20382172977994764, 0.20382172977994764, 0.20382172977994764, 0.20382172977994764, 0.4076434595598953, 0.2847107465795602, 0.25904896202068145, 0.12965954303433483, 0.11939482921078332, 0.2071851448069475, 0.40276918371572773, 0.13425639457190924, 0.13425639457190924, 0.13425639457190924, 0.13425639457190924, 0.3192096782392953, 0.20686534983879404, 0.16650418375017997, 0.1484508986336204, 0.1589149046565944, 0.3447914694395362, 0.21782356341510395, 0.17734104265369074, 0.12696790602443228, 0.13317829273214907, 0.20613216720560354, 0.4122643344112071, 0.10306608360280177, 0.10306608360280177, 0.20613216720560354, 0.34259424693411694, 0.34259424693411694, 0.34259424693411694, 0.2590044697446785, 0.14070665358362816, 0.22096155970169754, 0.14174892509165501, 0.23763790383012753, 0.34987774556716444, 0.17593883612655742, 0.21590405477671537, 0.2471412371699423, 0.21498531411809108, 0.146079764721267, 0.5166583212361967, 0.14666060960595376, 0.2933212192119075, 0.14666060960595376, 0.14666060960595376, 0.2933212192119075, 0.1455511665317386, 0.25471454143054256, 0.2911023330634772, 0.1455511665317386, 0.1455511665317386, 0.36770696509880557, 0.21647813009975891, 0.1413734727182099, 0.1389945921676631, 0.13525635130251812, 0.235778336730078, 0.18933714919233538, 0.1604602569413031, 0.2170232417629127, 0.1973750470354062, 0.21918856900656114, 0.22787162795007587, 0.20792406010686634, 0.2065159964944045, 0.13822491129000483, 0.14840916310347066, 0.14840916310347066, 0.2968183262069413, 0.14840916310347066, 0.14840916310347066, 0.5153642975595762, 0.5125083463479533, 0.22782969830272817, 0.20835707451617022, 0.16746456456439848, 0.24535505971063035, 0.14993920315649634, 0.1873001293368563, 0.18542712804348774, 0.18823662998354057, 0.22241890358751684, 0.21679989970741118, 0.10413013543104467, 0.10413013543104467, 0.312390406293134, 0.20826027086208934, 0.20826027086208934, 0.263920592121693, 0.2245143781372136, 0.13579987588497527, 0.2093581419893369, 0.16611234818072868, 0.1692776432894483, 0.3808746974012587, 0.183384113563569, 0.11285176219296554, 0.14106470274120692, 0.31190996355143685, 0.22459657409758693, 0.1480903566227062, 0.1664946494977824, 0.14894637024480276, 0.23697189623255258, 0.3519284756815355, 0.12100692573577153, 0.1512586571697144, 0.14016635564393534, 0.37668799669322234, 0.21097024504110878, 0.16134854539386573, 0.1282674122957037, 0.12264986139224224, 0.5093664726877971, 0.15290763216072106, 0.2458031291185785, 0.26553314617157475, 0.1644168087749689, 0.1718155651698425, 0.45560365219398896, 0.2733621913163934, 0.09112073043879779, 0.09112073043879779, 0.1366810956581967, 0.23159905210186524, 0.31998481280196484, 0.13825045457101137, 0.15361161619001265, 0.15644752295044365, 0.08307903710191793, 0.6646322968153434, 0.08307903710191793, 0.08307903710191793, 0.16615807420383585, 0.2183067284802743, 0.28253416536547693, 0.16273169241898494, 0.21364774342124404, 0.12279753477015429, 0.22243427177241176, 0.22243427177241176, 0.11121713588620588, 0.11121713588620588, 0.4448685435448235, 0.16741068540325257, 0.16741068540325257, 0.16741068540325257, 0.16741068540325257, 0.33482137080650515, 0.34539723352607715, 0.34539723352607715, 0.3426601095592352, 0.33262565273145434, 0.33262565273145434, 0.33262565273145434, 0.19456647552275597, 0.354613092485023, 0.20398098240288934, 0.11925042048168914, 0.1286649273618225, 0.2670203089715271, 0.2670203089715271, 0.2670203089715271, 0.26882226354506933, 0.2106659210725723, 0.17543427791495997, 0.16517849137935364, 0.18001921777793692, 0.5041835228402356, 0.3461352729655601, 0.21379890348756458, 0.15390945160701186, 0.15970520501480728, 0.12654061607020012, 0.21186278762275265, 0.2024466637284081, 0.25141050797899983, 0.15348281947781636, 0.18173119116085007, 0.21110358968398035, 0.285417314401394, 0.12495228049830613, 0.2301752535495113, 0.14862744943482728, 0.20963414338266464, 0.20963414338266464, 0.1310213396141654, 0.2882469471511639, 0.1572256075369985, 0.23572147832501616, 0.3928691305416936, 0.15714765221667745, 0.07857382610833873, 0.15714765221667745, 0.20682288583245897, 0.20682288583245897, 0.20682288583245897, 0.20682288583245897, 0.26397420312779246, 0.24235825998972382, 0.1831626559897125, 0.16895571102970977, 0.14145839820389805, 0.4202475164638189, 0.24014143797932508, 0.12007071898966254, 0.12007071898966254, 0.12007071898966254, 0.23533413500381997, 0.21638613701639484, 0.1743215814843111, 0.21941781669438287, 0.15423670361764047, 0.18800976820534107, 0.2684390150942745, 0.1818623098444035, 0.21106273705885703, 0.15086887394134318, 0.25154087473505876, 0.2153019351545842, 0.26433108870463806, 0.12577043736752938, 0.14282405599363507, 0.5081782630928643, 0.5075673994291441, 0.2762429832966961, 0.25161135668406925, 0.17615631623540812, 0.1267512302273562, 0.1692065098782946, 0.23375514449418233, 0.29684160461190817, 0.1517732228919202, 0.20937390386897425, 0.10849652078217589, 0.2721430606137655, 0.22622212232564232, 0.1813262049814505, 0.15026807038033152, 0.17005097459820598, 0.5072776663227025, 0.32454441748018736, 0.21984785206205054, 0.1399439112650417, 0.16637942065022032, 0.14935316036824087, 0.32996253428141264, 0.19210012456072678, 0.20590608833819174, 0.14614598798716483, 0.12602872648285873, 0.33830333273432645, 0.2827837596435714, 0.11012398838331096, 0.16137282508246953, 0.10737851498871318, 0.23281360115935576, 0.2201331217933778, 0.1496296565185402, 0.21176400541183232, 0.18589582750523723, 0.2302224601975121, 0.2302224601975121, 0.2302224601975121, 0.3453336902962682, 0.11511123009875605, 0.14740121622022564, 0.14740121622022564, 0.14740121622022564, 0.2948024324404513, 0.14740121622022564, 0.16813233618811813, 0.16813233618811813, 0.08406616809405906, 0.16813233618811813, 0.33626467237623625, 0.25832784547750187, 0.25832784547750187, 0.25832784547750187, 0.25832784547750187, 0.2571736665589754, 0.22309569029146736, 0.15088544475999538, 0.17904634476240555, 0.18978474242869425, 0.15828485752565646, 0.2401100804838348, 0.1837714023814825, 0.24145147758150987, 0.17706441689310726, 0.09510016389295142, 0.19020032778590285, 0.28530049167885424, 0.09510016389295142, 0.19020032778590285, 0.13469153809754722, 0.24693448651216993, 0.15714012778047176, 0.291831665878019, 0.1795887174633963, 0.1718156083137482, 0.1718156083137482, 0.1718156083137482, 0.3436312166274964, 0.1718156083137482, 0.26081221904189084, 0.5216244380837817, 0.26081221904189084, 0.26081221904189084, 0.40756985996053524, 0.1358566199868451, 0.1358566199868451, 0.1358566199868451, 0.1358566199868451, 0.35065855540702245, 0.5100109294986405, 0.13006654157874373, 0.26013308315748745, 0.13006654157874373, 0.39019962473623115, 0.13006654157874373, 0.26582615074666044, 0.25150595521073804, 0.17048854643292494, 0.20607844416190854, 0.10610783541008051, 0.5113749535139597, 0.17045831783798657, 0.08522915891899328, 0.17045831783798657, 0.08522915891899328, 0.19182695570349853, 0.2380967741646439, 0.17061995557547355, 0.23713281961337004, 0.1619443646140088, 0.2159438140027522, 0.19420767517534168, 0.25894356690045556, 0.14033985286393305, 0.19042747711840072, 0.4724280558359929, 0.15747601861199761, 0.15747601861199761, 0.15747601861199761, 0.15747601861199761, 0.2208569757475367, 0.23116064804407493, 0.24034435596055467, 0.1292438894587512, 0.17852232218132533, 0.2160390984579129, 0.2615563479154843, 0.17736957679484333, 0.18408303541690402, 0.16085446858457403, 0.5195024074702184, 0.22607052412835763, 0.21571271126988706, 0.24048139419231668, 0.16257262790903806, 0.15536719287705852, 0.4963129672001705, 0.49289392353056977, 0.49289392353056977, 0.34685707323796167, 0.34685707323796167, 0.33424025693552706, 0.33424025693552706, 0.5076861091984892, 0.5134593139487721, 0.47522199189258063, 0.23761099594629032, 0.23761099594629032, 0.23761099594629032, 0.3297224976977869, 0.3297224976977869, 0.3297224976977869, 0.20876152512080798, 0.20876152512080798, 0.41752305024161596, 0.20876152512080798, 0.20876152512080798, 0.4776552089007863, 0.36710626108299493, 0.1842205964707393, 0.1555195615133415, 0.14283770885774713, 0.15017983407940702, 0.47506076751249243, 0.5107695714449917, 0.4735643521831655, 0.3472464305165804, 0.3472464305165804, 0.33510611425563736, 0.33510611425563736, 0.33510611425563736, 0.513737423509912, 0.5025589214783133, 0.18477866817927818, 0.2953460254062296, 0.24924505951160242, 0.12218629985080058, 0.14804781730388414, 0.20697057268439153, 0.20697057268439153, 0.20697057268439153, 0.41394114536878307, 0.20697057268439153, 0.33405287767009345, 0.20669865387376052, 0.16528562570401772, 0.1729818521780407, 0.12094070173464712, 0.13516406269601508, 0.6758203134800754, 0.13516406269601508, 0.13516406269601508, 0.14863248536832543, 0.19817664715776726, 0.19817664715776726, 0.29726497073665087, 0.14863248536832543, 0.4819222179321793, 0.1784897103452516, 0.14279176827620127, 0.14279176827620127, 0.07139588413810063, 0.21628149839842736, 0.31510518401706983, 0.14007001203150812, 0.1289732846209402, 0.19953172117492862, 0.3838663233430685, 0.25082396866759116, 0.15965082561057284, 0.08999923992752881, 0.11582510877629794, 0.513850792155461, 0.3783702668465556, 0.1891851334232778, 0.1891851334232778, 0.1891851334232778, 0.1891851334232778, 0.25460950970065394, 0.25460950970065394, 0.25460950970065394, 0.25460950970065394, 0.2324009933618242, 0.376268274966763, 0.15493399557454945, 0.12173385366571744, 0.12173385366571744, 0.22311599943769833, 0.1872452599782613, 0.27190020510253265, 0.1836581860323176, 0.13343915078910576, 0.3131658944420507, 0.18143522765593223, 0.1686695754106795, 0.17287952455539052, 0.16391640702148966, 0.14888414390095478, 0.14888414390095478, 0.29776828780190956, 0.14888414390095478, 0.14888414390095478, 0.22014603469780442, 0.2578853549317138, 0.24656355886154094, 0.15032829226507216, 0.12453975677190078, 0.21626338858380945, 0.23171077348265298, 0.15844603367670937, 0.22023557327208348, 0.17345206472130023, 0.20148955649853825, 0.20148955649853825, 0.20148955649853825, 0.20148955649853825, 0.4029791129970765, 0.22785300772416872, 0.19279869884352738, 0.1665079671830464, 0.2716708938249704, 0.1489808127427257, 0.3517979859335404, 0.1628455041426381, 0.20420309249632396, 0.11786912680800472, 0.16336247399705917, 0.6185597471503721], \"Term\": [\"aam\", \"adad\", \"adad\", \"adad\", \"adad\", \"adad\", \"aeonian\", \"aeonian\", \"aeonian\", \"aeonian\", \"aeonian\", \"aggressor\", \"aggressor\", \"aggressor\", \"aggressor\", \"aggressor\", \"agua\", \"agua\", \"agua\", \"agua\", \"agua\", \"ahunt\", \"air\", \"air\", \"air\", \"air\", \"air\", \"alimony\", \"alimony\", \"alimony\", \"alto\", \"alto\", \"alto\", \"alto\", \"alto\", \"amorousness\", \"ann\", \"ann\", \"ann\", \"ann\", \"ann\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"anthology\", \"anthology\", \"anthology\", \"anthology\", \"anthology\", \"antre\", \"antre\", \"antre\", \"antre\", \"antre\", \"articulation\", \"articulation\", \"articulation\", \"articulation\", \"articulation\", \"atavistic\", \"atavistic\", \"atavistic\", \"atavistic\", \"atavistic\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"automatically\", \"beast\", \"beast\", \"beast\", \"beast\", \"beast\", \"bedizen\", \"bedizen\", \"bedizen\", \"bedizen\", \"bedizen\", \"beg\", \"beg\", \"beg\", \"beg\", \"beg\", \"believing\", \"believing\", \"believing\", \"believing\", \"believing\", \"betray\", \"betray\", \"betray\", \"betray\", \"betray\", \"beware\", \"beware\", \"beware\", \"beware\", \"beware\", \"black\", \"black\", \"black\", \"black\", \"black\", \"blacken\", \"blacken\", \"blacken\", \"blacken\", \"blacken\", \"blast\", \"blast\", \"blast\", \"blast\", \"blast\", \"blench\", \"blood\", \"blood\", \"blood\", \"blood\", \"blood\", \"boden\", \"body\", \"body\", \"body\", \"body\", \"body\", \"bor\", \"bor\", \"bor\", \"bor\", \"bor\", \"born\", \"born\", \"born\", \"born\", \"born\", \"bramble\", \"bramble\", \"break\", \"break\", \"break\", \"break\", \"break\", \"breath\", \"breath\", \"breath\", \"breath\", \"breath\", \"bring\", \"bring\", \"bring\", \"bring\", \"bring\", \"bruin\", \"bruin\", \"bruin\", \"bruin\", \"bruin\", \"buggy\", \"buggy\", \"buggy\", \"buggy\", \"buggy\", \"build\", \"build\", \"build\", \"build\", \"build\", \"burn\", \"burn\", \"burn\", \"burn\", \"burn\", \"burning\", \"burning\", \"burning\", \"burning\", \"burning\", \"calor\", \"calor\", \"caza\", \"centripetal\", \"centripetal\", \"centripetal\", \"centripetal\", \"centripetal\", \"chance\", \"chance\", \"chance\", \"chance\", \"chance\", \"change\", \"change\", \"change\", \"change\", \"change\", \"chaos\", \"chaos\", \"chaos\", \"chaos\", \"chaos\", \"chasse\", \"chasse\", \"choose\", \"choose\", \"choose\", \"choose\", \"choose\", \"church\", \"church\", \"church\", \"church\", \"church\", \"city\", \"city\", \"city\", \"city\", \"city\", \"cleavage\", \"cleavage\", \"cleavage\", \"cleavage\", \"cleavage\", \"coaxial\", \"coaxial\", \"coaxial\", \"coaxial\", \"coaxial\", \"coffin\", \"coffin\", \"coffin\", \"coffin\", \"coffin\", \"colation\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"colors\", \"colors\", \"colors\", \"colors\", \"colors\", \"colporrhagia\", \"comedic\", \"comfort\", \"comfort\", \"comfort\", \"comfort\", \"comfort\", \"coming\", \"coming\", \"coming\", \"coming\", \"coming\", \"condivision\", \"condivision\", \"contortedly\", \"contortedly\", \"control\", \"control\", \"control\", \"control\", \"control\", \"coroner\", \"coroner\", \"coroner\", \"coroner\", \"coroner\", \"corpse\", \"corpse\", \"corpse\", \"corpse\", \"corpse\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"crush\", \"crush\", \"crush\", \"crush\", \"crush\", \"culpable\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"dado\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"darkness\", \"darkness\", \"darkness\", \"darkness\", \"darkness\", \"dawn\", \"dawn\", \"dawn\", \"dawn\", \"dawn\", \"day\", \"day\", \"day\", \"day\", \"day\", \"days\", \"days\", \"days\", \"days\", \"days\", \"dead\", \"dead\", \"dead\", \"dead\", \"dead\", \"death\", \"death\", \"death\", \"death\", \"death\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"desire\", \"desire\", \"desire\", \"desire\", \"desire\", \"desireless\", \"desireless\", \"desireless\", \"desireless\", \"desireless\", \"dinero\", \"dinero\", \"dinero\", \"dinero\", \"dinero\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"dissimulate\", \"dissimulate\", \"dissimulate\", \"dissimulate\", \"dissimulate\", \"district\", \"district\", \"district\", \"district\", \"district\", \"dolor\", \"dolor\", \"dolor\", \"dolor\", \"dolor\", \"domine\", \"domine\", \"domine\", \"domine\", \"domine\", \"doom\", \"doom\", \"doom\", \"doom\", \"doom\", \"dragonhead\", \"dragonhead\", \"dragonhead\", \"dragonhead\", \"dragonhead\", \"dream\", \"dream\", \"dream\", \"dream\", \"dream\", \"druidess\", \"druidess\", \"druidess\", \"druidess\", \"druidess\", \"dry\", \"dry\", \"dry\", \"dry\", \"dry\", \"ectoplasmic\", \"ectoplasmic\", \"ectoplasmic\", \"ectoplasmic\", \"ectoplasmic\", \"electrolytic\", \"epigenetic\", \"epigenetic\", \"epigenetic\", \"epigenetic\", \"epigenetic\", \"erase\", \"erase\", \"erase\", \"erase\", \"erase\", \"essentially\", \"essentially\", \"essentially\", \"essentially\", \"essentially\", \"eternal\", \"eternal\", \"eternal\", \"eternal\", \"eternal\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"fading\", \"fading\", \"fading\", \"fading\", \"fading\", \"fairy\", \"fairy\", \"fairy\", \"fairy\", \"fairy\", \"faith\", \"faith\", \"faith\", \"faith\", \"faith\", \"fall\", \"fall\", \"fall\", \"fall\", \"fall\", \"fand\", \"fand\", \"fand\", \"fand\", \"fand\", \"father\", \"father\", \"father\", \"father\", \"father\", \"fear\", \"fear\", \"fear\", \"fear\", \"fear\", \"feast\", \"feast\", \"feast\", \"feast\", \"feast\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"field\", \"field\", \"field\", \"field\", \"field\", \"fiesta\", \"fiesta\", \"fiesta\", \"fiesta\", \"fife\", \"fife\", \"fife\", \"fife\", \"fife\", \"fight\", \"fight\", \"fight\", \"fight\", \"fight\", \"final\", \"final\", \"final\", \"final\", \"final\", \"flame\", \"flame\", \"flame\", \"flame\", \"flame\", \"flesh\", \"flesh\", \"flesh\", \"flesh\", \"flesh\", \"flowing\", \"flowing\", \"flowing\", \"flowing\", \"flowing\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"freeman\", \"freeman\", \"freeman\", \"freeman\", \"freeman\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"fuerte\", \"fuerte\", \"fuerte\", \"fuerte\", \"fuerte\", \"fulguration\", \"gate\", \"gate\", \"gate\", \"gate\", \"gate\", \"ghost\", \"ghost\", \"ghost\", \"ghost\", \"ghost\", \"gimmick\", \"gimmick\", \"gimmick\", \"gimmick\", \"gimmick\", \"gladius\", \"gladius\", \"gladius\", \"gladius\", \"gladius\", \"glimpse\", \"glimpse\", \"glimpse\", \"glimpse\", \"glimpse\", \"god\", \"god\", \"god\", \"god\", \"god\", \"golpe\", \"golpe\", \"golpe\", \"grab\", \"grab\", \"grab\", \"grab\", \"grab\", \"grave\", \"grave\", \"grave\", \"grave\", \"grave\", \"greyness\", \"greyness\", \"greyness\", \"greyness\", \"greyness\", \"griddle\", \"guardo\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"handbreadth\", \"handbreadth\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"harem\", \"harem\", \"harem\", \"harem\", \"harem\", \"hate\", \"hate\", \"hate\", \"hate\", \"hate\", \"head\", \"head\", \"head\", \"head\", \"head\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heaven\", \"heaven\", \"heaven\", \"heaven\", \"heaven\", \"hedgehog\", \"hedgehog\", \"hedgehog\", \"hedgehog\", \"hemic\", \"hero\", \"hero\", \"hero\", \"hero\", \"hero\", \"hew\", \"hew\", \"hew\", \"hew\", \"hew\", \"hexagon\", \"hexagon\", \"hexagon\", \"hexagon\", \"hexagon\", \"hieron\", \"hold\", \"hold\", \"hold\", \"hold\", \"hold\", \"holy\", \"holy\", \"holy\", \"holy\", \"holy\", \"hoot\", \"hoot\", \"hoot\", \"hoot\", \"hoot\", \"hope\", \"hope\", \"hope\", \"hope\", \"hope\", \"howdy\", \"howdy\", \"howdy\", \"howdy\", \"howdy\", \"hoy\", \"hoy\", \"hoy\", \"hoy\", \"hoy\", \"human\", \"human\", \"human\", \"human\", \"human\", \"hunchback\", \"hunchback\", \"hunchback\", \"hunchback\", \"hunchback\", \"hypocrite\", \"hypocrite\", \"hypocrite\", \"hypocrite\", \"hypocrite\", \"illusion\", \"illusion\", \"illusion\", \"illusion\", \"illusion\", \"impassiveness\", \"incalculably\", \"inthronization\", \"iron\", \"iron\", \"iron\", \"iron\", \"iron\", \"irreligious\", \"irreligious\", \"irreligious\", \"irreligious\", \"irreligious\", \"irritant\", \"izar\", \"izar\", \"izar\", \"jazz\", \"jazz\", \"jazz\", \"jazz\", \"jazz\", \"kat\", \"kat\", \"kat\", \"kat\", \"kat\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"killing\", \"killing\", \"killing\", \"killing\", \"killing\", \"king\", \"king\", \"king\", \"king\", \"king\", \"kingdom\", \"kingdom\", \"kingdom\", \"kingdom\", \"kingdom\", \"kore\", \"kore\", \"kore\", \"kore\", \"land\", \"land\", \"land\", \"land\", \"land\", \"laugh\", \"laugh\", \"laugh\", \"laugh\", \"laugh\", \"left\", \"left\", \"left\", \"left\", \"left\", \"legion\", \"legion\", \"legion\", \"legion\", \"legion\", \"lehr\", \"lie\", \"lie\", \"lie\", \"lie\", \"lie\", \"life\", \"life\", \"life\", \"life\", \"life\", \"light\", \"light\", \"light\", \"light\", \"light\", \"linge\", \"live\", \"live\", \"live\", \"live\", \"live\", \"living\", \"living\", \"living\", \"living\", \"living\", \"lost\", \"lost\", \"lost\", \"lost\", \"lost\", \"love\", \"love\", \"love\", \"love\", \"love\", \"lullaby\", \"lullaby\", \"lullaby\", \"lullaby\", \"lullaby\", \"manticore\", \"manticore\", \"manticore\", \"manticore\", \"manticore\", \"matter\", \"matter\", \"matter\", \"matter\", \"matter\", \"meddlesome\", \"meddlesome\", \"meddlesome\", \"meddlesome\", \"meddlesome\", \"metal\", \"metal\", \"metal\", \"metal\", \"metal\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"methodology\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirror\", \"mirthful\", \"mischance\", \"misgovernment\", \"misgovernment\", \"misgovernment\", \"misgovernment\", \"money\", \"money\", \"money\", \"money\", \"money\", \"moonlight\", \"moonlight\", \"moonlight\", \"moonlight\", \"moonlight\", \"mou\", \"mou\", \"mou\", \"mou\", \"mou\", \"mountainside\", \"mountainside\", \"mountainside\", \"mountainside\", \"mountainside\", \"murder\", \"murder\", \"murder\", \"murder\", \"murder\", \"myst\", \"myst\", \"myst\", \"myst\", \"myst\", \"nauseousness\", \"news\", \"news\", \"news\", \"news\", \"news\", \"night\", \"night\", \"night\", \"night\", \"night\", \"nightmare\", \"nightmare\", \"nightmare\", \"nightmare\", \"nightmare\", \"nihilist\", \"nihilist\", \"nihilist\", \"nihilist\", \"nihilist\", \"obstructive\", \"orlo\", \"orlo\", \"orlo\", \"overrule\", \"overrule\", \"overrule\", \"overrule\", \"overrule\", \"pain\", \"pain\", \"pain\", \"pain\", \"pain\", \"parcel\", \"parcel\", \"parcel\", \"parcel\", \"parcel\", \"peace\", \"peace\", \"peace\", \"peace\", \"peace\", \"pee\", \"pee\", \"pee\", \"pee\", \"pee\", \"perorate\", \"perorate\", \"perorate\", \"pimply\", \"pinta\", \"pinta\", \"placer\", \"placer\", \"placer\", \"placer\", \"placer\", \"play\", \"play\", \"play\", \"play\", \"play\", \"pollen\", \"pollen\", \"pollen\", \"pollen\", \"pollen\", \"power\", \"power\", \"power\", \"power\", \"power\", \"pray\", \"pray\", \"pray\", \"pray\", \"pray\", \"precedence\", \"precedence\", \"precedence\", \"precedence\", \"precedence\", \"puist\", \"puist\", \"puist\", \"pull\", \"pull\", \"pull\", \"pull\", \"pull\", \"puna\", \"pure\", \"pure\", \"pure\", \"pure\", \"pure\", \"quatre\", \"quirk\", \"quirk\", \"quirk\", \"quirk\", \"quirk\", \"rais\", \"rais\", \"rais\", \"rais\", \"rais\", \"ready\", \"ready\", \"ready\", \"ready\", \"ready\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"red\", \"red\", \"red\", \"red\", \"red\", \"ree\", \"ree\", \"ree\", \"ree\", \"ree\", \"refrigerate\", \"reinsertion\", \"reject\", \"reject\", \"reject\", \"reject\", \"reject\", \"remain\", \"remain\", \"remain\", \"remain\", \"remain\", \"resuscitation\", \"resuscitation\", \"resuscitation\", \"resuscitation\", \"resuscitation\", \"return\", \"return\", \"return\", \"return\", \"return\", \"ripper\", \"ripper\", \"ripper\", \"ripper\", \"ripper\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rite\", \"rite\", \"rite\", \"rite\", \"rite\", \"rock\", \"rock\", \"rock\", \"rock\", \"rock\", \"rollick\", \"rotting\", \"rotting\", \"rotting\", \"rotting\", \"rotting\", \"rufus\", \"rufus\", \"rufus\", \"rufus\", \"rufus\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rumbo\", \"rumbo\", \"rumbo\", \"rumbo\", \"rumbo\", \"sacrifice\", \"sacrifice\", \"sacrifice\", \"sacrifice\", \"sacrifice\", \"sah\", \"sah\", \"sah\", \"sah\", \"sah\", \"sailorman\", \"sailorman\", \"sailorman\", \"sailorman\", \"sailorman\", \"sant\", \"sant\", \"sarcous\", \"sayid\", \"sayid\", \"sayid\", \"school\", \"school\", \"school\", \"school\", \"school\", \"senicide\", \"senicide\", \"senicide\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shapelessness\", \"shine\", \"shine\", \"shine\", \"shine\", \"shine\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"shore\", \"shore\", \"shore\", \"shore\", \"shore\", \"sigil\", \"sigil\", \"sigil\", \"sigil\", \"sigil\", \"signum\", \"signum\", \"signum\", \"signum\", \"signum\", \"skrike\", \"skrike\", \"skrike\", \"skrike\", \"sky\", \"sky\", \"sky\", \"sky\", \"sky\", \"skylark\", \"skylark\", \"skylark\", \"skylark\", \"skylark\", \"slave\", \"slave\", \"slave\", \"slave\", \"slave\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"snake\", \"snake\", \"snake\", \"snake\", \"snake\", \"soldering\", \"solipsism\", \"soul\", \"soul\", \"soul\", \"soul\", \"soul\", \"spirit\", \"spirit\", \"spirit\", \"spirit\", \"spirit\", \"stand\", \"stand\", \"stand\", \"stand\", \"stand\", \"stapler\", \"star\", \"star\", \"star\", \"star\", \"star\", \"start\", \"start\", \"start\", \"start\", \"start\", \"steel\", \"steel\", \"steel\", \"steel\", \"steel\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stork\", \"stork\", \"stork\", \"stork\", \"stork\", \"streamlet\", \"streamlet\", \"streamlet\", \"streamlet\", \"streamlet\", \"subdivide\", \"subdivide\", \"subdivide\", \"subdivide\", \"subdivide\", \"subjectively\", \"subjectively\", \"subjectively\", \"subjectively\", \"sun\", \"sun\", \"sun\", \"sun\", \"sun\", \"surface\", \"surface\", \"surface\", \"surface\", \"surface\", \"tartar\", \"tartar\", \"tartar\", \"tartar\", \"tartar\", \"taunt\", \"taunt\", \"taunt\", \"taunt\", \"taunt\", \"teratoma\", \"teratoma\", \"teratoma\", \"teratoma\", \"teratoma\", \"th\", \"th\", \"th\", \"th\", \"thankfully\", \"thankfully\", \"thankfully\", \"thankfully\", \"thankfully\", \"thereabove\", \"thujone\", \"tideless\", \"tideless\", \"tideless\", \"tideless\", \"tideless\", \"time\", \"time\", \"time\", \"time\", \"time\", \"totum\", \"totum\", \"totum\", \"totum\", \"totum\", \"tower\", \"tower\", \"tower\", \"tower\", \"tower\", \"trap\", \"trap\", \"trap\", \"trap\", \"trap\", \"triplicity\", \"triplicity\", \"triplicity\", \"triplicity\", \"triplicity\", \"true\", \"true\", \"true\", \"true\", \"true\", \"truth\", \"truth\", \"truth\", \"truth\", \"truth\", \"turma\", \"twist\", \"twist\", \"twist\", \"twist\", \"twist\", \"ultimo\", \"unbeheld\", \"unbeheld\", \"unbenign\", \"unbenign\", \"unburnt\", \"unburnt\", \"unchangeability\", \"undigestable\", \"undreaming\", \"undreaming\", \"undreaming\", \"undreaming\", \"unearned\", \"unearned\", \"unearned\", \"unembalmed\", \"unembalmed\", \"unembalmed\", \"unembalmed\", \"unembalmed\", \"unfaith\", \"unite\", \"unite\", \"unite\", \"unite\", \"unite\", \"unrighteousness\", \"untested\", \"unwire\", \"upholster\", \"upholster\", \"uta\", \"uta\", \"uta\", \"ventrotomy\", \"vesperal\", \"victim\", \"victim\", \"victim\", \"victim\", \"victim\", \"vitiation\", \"vitiation\", \"vitiation\", \"vitiation\", \"vitiation\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"volar\", \"volar\", \"volar\", \"volar\", \"voluntary\", \"voluntary\", \"voluntary\", \"voluntary\", \"voluntary\", \"wa\", \"wa\", \"wa\", \"wa\", \"wa\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"warrior\", \"warrior\", \"warrior\", \"warrior\", \"warrior\", \"washtub\", \"wayless\", \"wayless\", \"wayless\", \"wayless\", \"wayless\", \"welsh\", \"welsh\", \"welsh\", \"welsh\", \"whack\", \"whack\", \"whack\", \"whack\", \"whack\", \"wheel\", \"wheel\", \"wheel\", \"wheel\", \"wheel\", \"wind\", \"wind\", \"wind\", \"wind\", \"wind\", \"windup\", \"windup\", \"windup\", \"windup\", \"windup\", \"wisdom\", \"wisdom\", \"wisdom\", \"wisdom\", \"wisdom\", \"word\", \"word\", \"word\", \"word\", \"word\", \"wort\", \"wort\", \"wort\", \"wort\", \"wort\", \"wraith\", \"wraith\", \"wraith\", \"wraith\", \"wraith\", \"wrong\", \"wrong\", \"wrong\", \"wrong\", \"wrong\", \"yom\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 5, 3, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1883620119029663449523297467\", ldavis_el1883620119029663449523297467_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1883620119029663449523297467\", ldavis_el1883620119029663449523297467_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1883620119029663449523297467\", ldavis_el1883620119029663449523297467_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1      0.005387 -0.002128       1        1  25.968287\n",
       "0      0.001482  0.002940       2        1  23.984369\n",
       "4     -0.001781  0.000885       3        1  17.216673\n",
       "2     -0.001855  0.002323       4        1  16.454075\n",
       "3     -0.003233 -0.004019       5        1  16.376597, topic_info=      Term          Freq         Total Category  logprob  loglift\n",
       "328   time  33239.000000  33239.000000  Default  30.0000  30.0000\n",
       "211   mind  19186.000000  19186.000000  Default  29.0000  29.0000\n",
       "72   death  23120.000000  23120.000000  Default  28.0000  28.0000\n",
       "29   blood  20753.000000  20753.000000  Default  27.0000  27.0000\n",
       "23   black  12982.000000  12982.000000  Default  26.0000  26.0000\n",
       "..     ...           ...           ...      ...      ...      ...\n",
       "38    burn   1629.245545   9998.954775   Topic5  -5.8406  -0.0050\n",
       "290    sky   1852.496355  13092.188400   Topic5  -5.7122  -0.1462\n",
       "85   dream   2027.214368  15799.118507   Topic5  -5.6220  -0.2440\n",
       "34   break   1717.776054  11513.556534   Topic5  -5.7877  -0.0932\n",
       "193   live   1665.676687  12364.485136   Topic5  -5.8185  -0.1953\n",
       "\n",
       "[493 rows x 6 columns], token_table=       Topic      Freq   Term\n",
       "term                         \n",
       "25626      1  0.598709    aam\n",
       "24540      1  0.204873   adad\n",
       "24540      2  0.204873   adad\n",
       "24540      3  0.204873   adad\n",
       "24540      4  0.204873   adad\n",
       "...      ...       ...    ...\n",
       "1543       2  0.162846  wrong\n",
       "1543       3  0.204203  wrong\n",
       "1543       4  0.117869  wrong\n",
       "1543       5  0.163362  wrong\n",
       "20231      1  0.618560    yom\n",
       "\n",
       "[1436 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1, 5, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./lda_results/ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './lda_results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metallyrics]",
   "language": "python",
   "name": "conda-env-metallyrics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
